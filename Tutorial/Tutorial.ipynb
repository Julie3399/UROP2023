{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of This Tutorial\n",
    "For anyone who want to use our scripts to continue the project, here is the manual of our code. Most of our code can be stoped by pressing 's' on the keyboard.\n",
    "\n",
    "# Yolov8 Model\n",
    "This model is to detect the circuit components and return all the needed information such as the names and the positions of the components. We are using the **Roboflow** website combiing with a colab tutorial to train the data.\n",
    "- **colabtutorial**: https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb This is the original file of the notebook, in order to use this notebok for training, something should be modified(will explain later)\n",
    "- **Roboflow**: We use this website to create the training data, by uploading pictures of the circuits and labeling the components. Please contact Joy to add you into the workspace so that you can use the training data created by Joy and Julie. After fully understand how it works, you are encouraged to create your own workspace and your own training data as the outcome really depends on the training image(light condition, image quality and filming angle .etc) and the quality of labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab tutorial:\n",
    "This tutorial will first show you an example telling you what can this yolov8 model do, feel free to skip. Then it will tell you how to use the **roboflow** to create your own custom dataset. Then you shall see something like this:\n",
    "```python\n",
    "!mkdir {HOME}/datasets\n",
    "%cd {HOME}/datasets\n",
    "\n",
    "!pip install roboflow --quiet\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
    "project = rf.workspace(\"roboflow-jvuqo\").project(\"football-players-detection-3zvbc\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n",
    "```\n",
    "Should be modified into the following if you are using our dataset:\n",
    "```python\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"kycCPEnB6cQ4clHmnMlE\")\n",
    "project = rf.workspace().project(\"test_1_with_board\")\n",
    "dataset = project.version(8).download(\"yolov8\") # change the version everytime you modify th edataset\n",
    "```\n",
    "If you are not very sure about the current version of the dataset, go to the workspace of the roboflow website and then check with the latest version of **Test_1_with_board Image Dataset**. If you decide to use your own custom dataset, after creating your dataset, go to the *Deploy* page of the website and then copy&paste:\n",
    "\n",
    "![Alt Text](pictures/tutorial_pic1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can train the model, you can then run the rest of the cell to see the output, but the output might seem to be really bad. That is because one can not delete pictures from the roboflow but only add more pictures or modify the labeling. So we put all the unwanted pictures into the test set in order to keep it away from the training set. Then all you need to do is to deploy the model by running the following cell:\n",
    "```python\n",
    "project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"{HOME}/runs/detect/train/\")\n",
    "```\n",
    "Then use the \"model_path\" to find the file named \"best.pt\" and download it, usually under the /runs/detect/train/weights. May named in best1.pt or best2.pt. Please make sure you have download the right best weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roboflow\n",
    "We only use this website to generate custom dataset. One should note that once you create a dataset, you can not remove pictures from it eventhough you may think that some of the pictures will influence the training outcome in a bad way. So be carefull with your mian custom dataset. One trick that you can use is to allowcate all the bad pictures into the \"test set\". The website will automatically split the whole data set into 3 parts: training set, validation set and test set and you have the right to modify it.\n",
    "\n",
    "Training dataset are always very important, so when you take the pictures for trainning your model, make sure that everything (filming angle/height, light condition ...) is under the same condition as in the final experiment.\n",
    "\n",
    "Also, the way you label the components really matters. That is to say, when you crop the shape of a cirtain component, you should make it as accurate as possible as a little difference on the location will influence the outcome very hugely. You are welcomed to follow our pattern to label them or using your own way. However, the boundary of the components should be very accurate when you are labeling them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of labelling\n",
    "\n",
    "| Classes | Overview | Overlap | Bad-behaved part | Buzzer |\n",
    "|:---------:|:------------------:|:-----------------------------------:|:---------:|:---------:|\n",
    "| <img src=\"pictures/classes.png\" alt=\"Classes\" width=\"200\"/> | <img src=\"pictures/boundary.png\" alt=\"Overview\" width=\"200\"/> | <img src=\"pictures/overlap.png\" alt=\"Overlap\" width=\"200\"/> | <img src=\"pictures/not well.png\" alt=\"Bad-behaved part\" width=\"200\"/> | <img src=\"pictures/buzzer.png\" alt=\"Buzzer\" width=\"200\"/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first picture represents the classes that we have been defines for this project, which are just the names of the electronic components.\n",
    "- As,you can see from the second picture, which is an overview of how it is looked like after labelling, the boundaries of the bounding boxes should be as accurate as possible since our code for circuit model is very sensitive to the boundaries in order to get accurate locations.\n",
    "- It is okay to have the boundaries overlap.\n",
    "- The bad-behaved part actually means the overlaping of wires. When the wires overlap a lot in a small space, the small part of the wire lies between the two parallel '2-wire' may not be recognized by the yolov8 model, which may lead to a disconnection error of the circuit during detection. \n",
    "- Some of the electronic components are of wierd shape, like buzzer, so in the last picture one can see that the bounding box only fits to the longer edges of the buzzer and this actually works well.\n",
    "- When the accuracy for certain component is not very ideal, you should add more pictures of this components. That is to say, in order to have better validation output, we should increase the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Model\n",
    "Here are 2 parts of the speaker model. One is origin from the github repository: https://github.com/zachlatta/openai-whisper-speaker-identification/blob/main/transcripts_with_speaker_names.ipynb and we modified a litle bit. Another is to use Azure from Microsoft.\n",
    "\n",
    "## Azure\n",
    "Before using the Azure, here is a few warnings.\n",
    "\n",
    "- Create your own account of Azure to get your API keys and region key.\n",
    "- Better to use Linux than using Macos: We have to download the SpeechSDK. The SpeechSDK for Macos is a combinition of ArmX86 and X86, and it only contains the name of the required library but no contents with it. Another difficulty is that you should link to the dynamic libraries yourself and maybe you can do that.\n",
    "- Lack of SpeechSDK in Python version. The 2 main functions that we want to use through Azure are:\n",
    "    - **Speaker Recognition**\n",
    "    - **Real Time Diarization** (currently under public review)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a programming language or tool:\n",
    "|  | C# | C++ | Go | Java | JavaScript | Objective-C | Python | Swift | CLI | REST |\n",
    "| -------- | -------- | -------- |-------- |-------- |-------- |-------- |-------- |-------- |-------- |-------- |\n",
    "| **Speaker Recog** | &#10003; | &#10003; | &#10003; | &#10007;| &#10003;| &#10007;| &#10007;| &#10007; | &#10007; | &#10003;|\n",
    "| **Real Time Diar** | &#10003; | &#10003; | &#10007; | &#10003;| &#10003;| &#10007;| &#10003;| &#10007; | &#10007; | &#10007;|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Python is my main coding Language, since the current SDK do not support the the above two functions in Python, I tried to use them in C++ with linux (tried C++ with Macos first but don't know how to link to the dynamic libraries) and still faced some problems:\n",
    "- The current code can be compiled but still has loads of bugs. Code is from: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speaker-recognition?tabs=script&pivots=programming-language-cpp I have never used C++ before so it is really hard for me to debug so I may leave this to you if you are familiar with C++.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the Azure:\n",
    "\n",
    "- Create an account with the Microsoft Azure Service: https://azure.microsoft.com/en-us/free/ai-services and then sign in with the Azure portal.\n",
    "- Inside the protal, create a speech service resource to get your own keys.\n",
    "- Then you can feel free to use the services locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Our code\n",
    "Once again, our code is based on the github repository https://github.com/zachlatta/openai-whisper-speaker-identification/blob/main/transcripts_with_speaker_names.ipynb. Currently, we cannot use it for real time diarization, it can only take a audio file as input then the output will be a txt file telling you 'who speak what'. The format will be like the following:<br>\n",
    "- speaker 1: hello.....\n",
    "- speaker 2: hi......<br>\n",
    "\n",
    "Our current method is to record the real time audio for 40 seconds(can be adjusted), then put it into the dirization function meanwhile start to record audio for another 40 seconds. The disadvantage is that we will always have a 50-second-delay(include the processing time). **real_time_speaker_diart** and **transcripts_with_speaker_names** acctually using the same code from the github repository while the former one contains a recorder function inside the file and the latter one contains only the diarization function with all other helper function in seperate files. There are a few parameters that can be adjusted:\n",
    "\n",
    "```python\n",
    "num_speakers = 2 #@param {type:\"integer\"}\n",
    "\n",
    "language = 'English' #@param ['any', 'English']\n",
    "\n",
    "model_size = 'tiny' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
    "```\n",
    "As for now, setting the model size to 'tiny' is enough as we only processing audio file within 1 minutes.\n",
    "\n",
    "- **recorder**: contains a recorder function which will record the real time audio and save it as a file in the same directory for every 20 seconds (duration can be changed).\n",
    "- **check_for_new_file**: contains a function that will search for new files in the current directory and return the name of new-added files. we then can pass the new-added function into the 'transcripts_with_...' file to process it and store all transcripts into the file named **transcript.txt**.\n",
    "- **GPT**: Since we may need to find contribution of each participant through their conversation, we may use a language model to help us to estimate their ability based on the transcipt. So this is a function to pass the text in **initial_promp** together with the **transcript** in to ChatGPT and get the generated-output back. One should have a valid API key for ChatGPT with credit inside the account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function\n",
    "This section refers to the `color_picker.ipynb` of `Masking and Facial` inside the Circuit Model. This file is designed for getting the HSV value of any given color in order to create a filter on it. Why we need this function? As you may noticed that, for some elctronic components such as LED, FM... we need to specify the direction of the current. So for each 'directional component', we used some color tape to specify the input and output like the following:\n",
    "| Real img | Track bar | Stacked imgs |\n",
    "|:---------:|:------------------:|:-----------------------------------:|\n",
    "| <img src=\"pictures/tutorial_pic3.jpg\" alt=\"Real img\" width=\"200\"/> | <img src=\"pictures/trackbars.ipg.png\" alt=\"Track bar\" width=\"200\"/> | <img src=\"pictures/color_picker.ipg.png\" alt=\"YOLO Image\" width=\"600\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Real img**: As you can see from the picture that we have a yellow tape indicating the input of the LED and the orange(red) tape on the FM indicating the output. \n",
    "- **Track bar**: The sencon picture is a example of the tracking bar. When you run the code, the stacked image will overlap the tracking bar, so you have to remove the stacked image to see the tracking bar. You can use the track bar to adjust the HSV value and you can see the stacked image for a real time response on the masking. We then use the HSV value to get the location of the tapes and then know the input or output of the electronic component.\n",
    "- **Stacked img**: The stack-image contains 3 images in total: the orginal image, masking image and image-after-masking. In the above case, we want to only keep the orange object. So as you can see from the masking image, we only keep the orange object in white with all other part in black. And then the last image return the image with the masking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Circuit Detection and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Detection:\n",
    "1. We start by capturing frames from a webcam.\n",
    "2. Then, for each frame, we focus on the board by cropping it.\n",
    "3. Depending on whether there are hands on the board:\n",
    "    1. If hands are present, we don't create a virtual board. Instead, we keep track of the hand's movements over time. By measuring the distance between fingers and pieces, we can figure out who placed each piece.\n",
    "    2. If there are no hands on the board, we create a virtual representation of the board:\n",
    "        - First, we identify the board's edges by detecting the black color.\n",
    "        - If needed, we adjust the board's orientation.\n",
    "        - We mark the pegs on the board; these marks help us establish the board's coordinates. (For more details, check `virtual_board_all.py`.)\n",
    "        - To make this virtual board match the real one, we use YOLO, an object detection tool, to find the exact positions of pieces in the frame. We then convert these real coordinates into board coordinates, details in `pieces_location.pieceOnEachLocation`.\n",
    "\n",
    "| Raw Image | Image with pegs | Image after using YOLO | After converted to board coordinates |\n",
    "|:---------:|:------------------:|:-----------------:|:-----------------------------------:|\n",
    "| <img src=\"pictures/Real_coordinate_image.png\" alt=\"Raw Image\" width=\"200\"/> | <img src=\"pictures/Image_Pegs.png\" alt=\"After Drawing Pegs\" width=\"200\"/> | <img src=\"pictures/Image_YOLO.png\" alt=\"YOLO Image\" width=\"200\"/> | <img src=\"pictures/Box_coordinate_image.png\" alt=\"Board Coordinates\" width=\"200\"/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Verification:\n",
    "\n",
    "1. **Defining Pieces Classes** (in `pieces.py`):\n",
    "    - We start by defining classes for different types of pieces.\n",
    "    - Board: `self.pos` stores the list of pieces at this position.\n",
    "    - Each class has methods to add, remove, and set input/output connections and other attributes related to electrical circuits.\n",
    "    - For FM and Music Circuit, they require special connection skills, with ports labelled below: (note that we attach tapes on the pieces for distinguishing their direction, please refer to Helper Function section for more detail)\n",
    "    \n",
    "    <img src=\"pictures/Waves.jpeg\" alt=\"Raw Image\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Assigning IDs and Establishing Connections** (in `add_pieces.py` and `connections.py`):\n",
    "    - For every piece, we give it a unique ID.\n",
    "    - We also define connections for each piece at its ports. These connections are crucial for establishing how pieces can be linked on the board. For instance, at (i,j) postion of the board, if piece A has an input, and piece B has an output, this indicates that A and B can be connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Creating Skills and Tasks** (in `task.py`):\n",
    "    - Next, we create skills and tasks. Detailed information about this can be found in `task.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Testing and Observation** (in `all_tests.py`):\n",
    "    - Then, create test for each skill\n",
    "    - Additionally, we create observation vectors tailored to each task. These vectors capture relevant data for assessing task performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output:\n",
    "|                     | Piece Added Information                                     | Skills Information and Observation Vector                       |\n",
    "|---------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Example 1:**      | ![Image 1](pictures/Piece_Added.png \"Piece Added Image 1\") | ![Image 2](pictures/Observation_Vector.png \"Observation Vector Image 2\") |\n",
    "| **Example 2:**      | ![Image 1](pictures/Piece_Added2.png \"Piece Added Image 2\")| ![Image 2](pictures/Skills2.png \"Skills Image 2\")              |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areas for Improvement:\n",
    "\n",
    "**Sensitivity to Lighting Conditions:** \n",
    "- The current detection performance is influenced by varying lighting conditions. To enhance accuracy, it's advisable to expand the training dataset to include diverse lighting scenarios. The existing dataset, comprising fewer than 200 samples, could benefit from augmentation with more training samples. Increased data variety is likely to yield improved detection accuracy.\n",
    "\n",
    "**Handling Overlapping Pieces:** \n",
    "- Detecting components becomes challenging when multiple pieces overlap, particularly when several pieces are connected in parallel, the shadow they formed will make detection of the wires between them difficult. Addressing this challenge requires the creation of a more extensive training dataset that specifically includes cases of overlapping wires and pieces. While previous efforts have shown performance improvements with additional training data, further enhancements are possible in this area.\n",
    "\n",
    "**Custom Error Correction:** \n",
    "- The `round_to_integer_with_error` function serves a critical role in mitigating errors that can arise from imprecise drawings of the pieces' bounding boxes or improper placement of pieces. In such scenarios, there's a possibility that these pieces may seem to pass through neighboring pegs, leading to potential inaccuracies such as oversized pieces or incorrect connections.\n",
    "- It's important to note that this error correction method involves the incorporation of a customizable error rate. Fine-tuning this error rate is essential to ensure precise and reliable error correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Human Detection\n",
    "We have developed two models using MediaPipe (Website where more information can be found: https://developers.google.com/mediapipe/solutions/vision/hand_landmarker):\n",
    "\n",
    "1. Hand Detection Model:\n",
    "    - This model is designed to recognize whose hand it is by analyzing the direction of the fingers. It's important to note that this capability is currently effective when people are sitting face to face, where their fingers are pointing toward each other. \n",
    "    - For in-depth exploration and utilization of this hand detection model, you can refer to the code located in `cvzone_hand.py`. Additionally, discover its implementation and video storage procedures in the `gesture.ipynb` notebook.\n",
    "2. Holistic Model with Hand, Pose, and Face Detection:\n",
    "    - Our Holistic Model is built to provide a comprehensive understanding of human subjects. While there are opportunities for performance enhancement, the model's potential is substantial. It offers the capability to assimilate intricate details such as facial expressions and body language, which play pivotal roles in gauging emotions and confidence.\n",
    "    - For practical implementation, you can explore two primary functionalities in the `mediapipe.ipynb` notebook: single human detection and detection of two individuals using YOLO (which uses human identification + cropping regions of interest + deploying the Holistic Model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  How to Decide \"Who Add What\"                      | Performance on Overlapped Hands                  | Performance on Hands Closer to the Other Side                    |\n",
    "|------------------------------|------------------------------|------------------------------|\n",
    "|  <img src=\"pictures/Hand_On_Board.jpeg\" alt=\"Image 1\" width=\"350\"/>     | <img src=\"pictures/Overlapped_Hands.jpeg\" alt=\"Image 1\" width=\"250\"/>    | <img src=\"pictures/Hands_Closer_To_Other_Side.png\" alt=\"Image 1\" width=\"250\"/>    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Areas For Improvement:\n",
    "\n",
    "**1. Enhanced Hand Model Functionality:**\n",
    "- At present, our hand model performs reliably when tracking two individuals seated face-to-face. However, there are opportunities for improvement:\n",
    "    - Exploring the integration of a tracking mechanism to follow multiple (>2) individuals and allocate unique identifiers (IDs) to each person.\n",
    "    - Investigating methods to distinguish between individuals when they are seated side by side. This can be challenging due to variations in finger orientation when reaching toward objects at varying distances.\n",
    "\n",
    "**2. Integration of Hand Model into the Holistic Model:**\n",
    "\n",
    "- When individuals are seated face-to-face, achieving an optimal camera angle to capture both individuals' frontal views can be a challenging task. Introducing multiple cameras, however, brings about new challenges, such as dealing with an increased volume of information to process and potentially limiting user mobility due to the presence of more equipment.\n",
    "- The use of YOLO for person detection may encounter instability issues when individuals overlap with each other. I would recommend delving into advanced research on multiple people tracking as an alternative to human detection using YOLO."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
