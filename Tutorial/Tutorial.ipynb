{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of This Tutorial\n",
    "For anyone who want to use our scripts to continue the project, here is the manual of our code.\n",
    "\n",
    "# Yolov8 Model\n",
    "This model is to detect the circuit components and return all the needed information such as the names and the positions of the components. We are using the **Roboflow** website combiing with a colab tutorial to train the data.\n",
    "- **colabtutorial**: https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb This is the original file of the notebook, in order to use this notebok for training, something should be modified(will explain later)\n",
    "- **Roboflow**: We use this website to create the training data, by uploading pictures of the circuits and labeling the components. Please contact Joy to add you into the workspace so that you can use the training data created by Joy and Julie. After fully understand how it works, you are encouraged to create your own workspace and your own training data as the outcome really depends on the training image(light condition, image quality and filming angle .etc) and the quality of labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab tutorial:\n",
    "This tutorial will first show you an example telling you what can this yolov8 model do, feel free to skip. Then it will tell you how to use the **roboflow** to create your own custom dataset. Then you shall see something like this:\n",
    "```python\n",
    "!mkdir {HOME}/datasets\n",
    "%cd {HOME}/datasets\n",
    "\n",
    "!pip install roboflow --quiet\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
    "project = rf.workspace(\"roboflow-jvuqo\").project(\"football-players-detection-3zvbc\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n",
    "```\n",
    "Should be modified into the following if you are using our dataset:\n",
    "```python\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"kycCPEnB6cQ4clHmnMlE\")\n",
    "project = rf.workspace().project(\"test_1_with_board\")\n",
    "dataset = project.version(8).download(\"yolov8\") # change the version everytime you modify th edataset\n",
    "```\n",
    "If you are not very sure about the current version of the dataset, go to the workspace of the roboflow website and then check with the latest version of **Test_1_with_board Image Dataset**. If you decide to use your own custom dataset, after creating your dataset, go to the *Deploy* page of the website and then copy&paste:\n",
    "\n",
    "![Alt Text](pictures/tutorial_pic1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can train the model, you can then run the rest of the cell to see the output, but the output might seem to be really bad. That is because one can not delete pictures from the roboflow but only add more pictures or modify the labeling. So we put all the unwanted pictures into the test set in order to keep it away from the training set. Then all you need to do is to deploy the model by running the following cell:\n",
    "```python\n",
    "project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"{HOME}/runs/detect/train/\")\n",
    "```\n",
    "Then use the \"model_path\" to find the file named \"best.pt\" and download it, usually under the /runs/detect/train/weights. May named in best1.pt or best2.pt. Please make sure you have download the right best weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roboflow\n",
    "We only use this website to generate custom dataset. One should note that once you create a dataset, you can not remove pictures from it eventhough you may think that some of the pictures will influence the training outcome in a bad way. So be carefull with your mian custom dataset. One trick that you can use is to allowcate all the bad pictures into the \"test set\". The website will automatically split the whole data set into 3 parts: training set, validation set and test set and you have the right to modify it.\n",
    "\n",
    "Training dataset are always very important, so when you take the pictures for trainning your model, make sure that everything (filming angle/height, light condition ...) is under the same condition as in the final experiment.\n",
    "\n",
    "Also, the way you label the components really matters. That is to say, when you crop the shape of a cirtain component, you should make it as accurate as possible as a little difference on the location will influence the outcome very hugely. You are welcomed to follow our pattern to label them or using your own way. However, the boundary of the components should be very accurate when you are labeling them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of labelling\n",
    "\n",
    "![Alt Text](pictures/tutorial_pic2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The boundaries of the bounding boxes should be as accurate as possible since our code for circuit model is very sensitive to the boundaries.\n",
    "- It is okay to have the boundaries overlap.\n",
    "- When the accuracy for certain component is not very ideal, you should add more pictures of this components. That is to say, in order to have better validation output, we should increase the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Model\n",
    "Here are 2 parts of the speaker model. One is origin from the github repository: https://github.com/zachlatta/openai-whisper-speaker-identification/blob/main/transcripts_with_speaker_names.ipynb and we modified a litle bit. Another is to use Azure from Microsoft.\n",
    "\n",
    "## Azure\n",
    "Before using the Azure, here is a few warnings.\n",
    "\n",
    "- Create your own account of Azure to get your API keys and region key.\n",
    "- Better to use Linux than using Macos: We have to download the SpeechSDK. The SpeechSDK for Macos is a combinition of ArmX86 and X86, and it only contains the name of the required library but no contents with it. Another difficulty is that you should link to the dynamic libraries yourself and maybe you can do that.\n",
    "- Lack of SpeechSDK in Python version. The 2 main functions that we want to use through Azure are:\n",
    "    - Speaker Recognition\n",
    "    - Real Time Diarization (currently under public review)<br>\n",
    "\n",
    "Python is my main coding Language, since the current SDK do not support the the above two functions in Python, I tried to use them in C++ with linux (tried C++ with Macos first but don't know how to link to the dynamic libraries) and still faced some problems:\n",
    "- The current code can be compiled but still has loads of bugs. Code is from: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speaker-recognition?tabs=script&pivots=programming-language-cpp I have never used C++ before so it is really hard for me to debug so I may leave this to you if you are familiar with C++.\n",
    "\n",
    "## Our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Circuit Detection and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Detection:\n",
    "1. We start by capturing frames from a webcam.\n",
    "2. Then, for each frame, we focus on the board by cropping it.\n",
    "3. Depending on whether there are hands on the board:\n",
    "    1. If hands are present, we don't create a virtual board. Instead, we keep track of the hand's movements over time. By measuring the distance between fingers and pieces, we can figure out who placed each piece.\n",
    "    2. If there are no hands on the board, we create a virtual representation of the board:\n",
    "        - First, we identify the board's edges by detecting the black color.\n",
    "        - If needed, we adjust the board's orientation.\n",
    "        - We mark the pegs on the board; these marks help us establish the board's coordinates. (For more details, check `virtual_board_all.py`.)\n",
    "        - To make this virtual board match the real one, we use YOLO, an object detection tool, to find the exact positions of pieces in the frame. We then convert these real coordinates into board coordinates, details in `pieces_location.pieceOnEachLocation`.\n",
    "\n",
    "| Raw Image | Image with pegs | Image after using YOLO | After converted to board coordinates |\n",
    "|:---------:|:------------------:|:-----------------:|:-----------------------------------:|\n",
    "| <img src=\"pictures/Real_coordinate_image.png\" alt=\"Raw Image\" width=\"200\"/> | <img src=\"pictures/Image_Pegs.png\" alt=\"After Drawing Pegs\" width=\"200\"/> | <img src=\"pictures/Image_YOLO.png\" alt=\"YOLO Image\" width=\"200\"/> | <img src=\"pictures/Box_coordinate_image.png\" alt=\"Board Coordinates\" width=\"200\"/> |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Circuit Verification:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
