{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # draw detections from holistic model to opencv\n",
    "mp_holistic = mp.solutions.holistic # import our holistic model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Get Realtime Webcam Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameWidth = 640\n",
    "frameHeight = 480\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap.set(3, frameWidth)\n",
    "cap.set(4, frameHeight)\n",
    "cap.set(10, 150)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if success: \n",
    "        cv2.imshow(\"Raw Input\", img)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Make Detections from Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Holistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        \n",
    "        if success:\n",
    "            # Recolor Feed (because mediapipe uses RGB while cv2 uses BGR)\n",
    "            #image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Make Detections\n",
    "            results = holistic.process(image)\n",
    "            # print(results.face_landmarks)  # this will just be numbers/coordinates\n",
    "            \n",
    "            # the four landmarks we have:\n",
    "            # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "            \n",
    "            # Recolor image back to BGR for rendering, because cv2 loves BGR\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            \n",
    "            # Draw face landmarks\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                                      mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(0,0,245), thickness=1, circle_radius=1)) # FACE_CONNECTIONS: draw the line between the points\n",
    "            # Right hand\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                      mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2))\n",
    "            # Left Hand\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "            # Pose Detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(0,245,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "                        \n",
    "        \n",
    "            cv2.imshow('Result from Holistic Model', image)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 For Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 09:18:29.070 Python[46587:5651774] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-13 09:18:29.070 Python[46587:5651774] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-13 09:18:29.070 Python[46587:5651774] Text input context does not respond to _valueForTIProperty:\n",
      "2023-07-13 09:18:29.090 Python[46587:5651774] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-13 09:18:29.090 Python[46587:5651774] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-13 09:18:29.090 Python[46587:5651774] Text input context does not respond to _valueForTIProperty:\n",
      "2023-07-13 09:18:29.091 Python[46587:5651774] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-13 09:18:29.091 Python[46587:5651774] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-13 09:18:29.091 Python[46587:5651774] Text input context does not respond to _valueForTIProperty:\n",
      "2023-07-13 09:18:29.274 Python[46587:5651774] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-13 09:18:29.274 Python[46587:5651774] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-13 09:18:29.274 Python[46587:5651774] Text input context does not respond to _valueForTIProperty:\n",
      "2023-07-13 09:18:29.274 Python[46587:5651774] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-13 09:18:29.274 Python[46587:5651774] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-13 09:18:29.274 Python[46587:5651774] Text input context does not respond to _valueForTIProperty:\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Function to detect grabbing and putting down actions\n",
    "def detect_gesture(hand_landmarks):\n",
    "    # Example logic for detecting grabbing gesture\n",
    "    thumb_tip = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.THUMB_TIP]\n",
    "    index_finger_tip = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    \n",
    "    distance = cv2.norm((thumb_tip.x, thumb_tip.y), (index_finger_tip.x, index_finger_tip.y))\n",
    "    \n",
    "    if distance < 0.1:\n",
    "        return \"Grabbing\"\n",
    "    else:\n",
    "        return \"Putting Down\"\n",
    "\n",
    "# Initialize Mediapipe hands module\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=False, max_num_hands=2)\n",
    "\n",
    "# Open video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    # Flip the frame horizontally for a mirrored view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Convert the frame to RGB for Mediapipe processing\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame with Mediapipe hands\n",
    "    results = mp_hands.process(rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            gesture = detect_gesture(hand_landmarks)\n",
    "            \n",
    "            # Draw landmarks and gesture text on the frame\n",
    "            mp.solutions.drawing_utils.draw_landmarks(\n",
    "                frame, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "            cv2.putText(frame, gesture, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "    \n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Multiple Poses Estimation\n",
    "\n",
    "Idea:\n",
    "- Use Mediapipe for single person detection\n",
    "- Use YOLO to detect person and crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "  \n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    cv2.imshow('MediaPipe Pose', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe with YOLO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/julie3399/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-7-17 Python-3.11.1 torch-2.1.0.dev20230328 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# version 1 (raw input and just object detection)\n",
    "\n",
    "# Load the model from torch.hub\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    " \n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if success:\n",
    "            # Make Detections\n",
    "            results = model(image) \n",
    "            cv2.imshow('Result',np.squeeze(results.render()))\n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "              break\n",
    "cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/julie3399/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-7-17 Python-3.11.1 torch-2.1.0.dev20230328 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics/yolov5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov5s\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiple.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misOpened\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      8\u001b[0m         success, image \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# version 2 (object detection with pose estimation from Mediapipe)\n",
    "# doesn't really work, cannot separate the two people: because we need to act directly on the frame, not onto the cropped ones\n",
    "\n",
    "# Load the model from torch.hub\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    " \n",
    "cap = cv2.VideoCapture('Multiple.mp4')\n",
    "while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if success:\n",
    "            output_frame = image.copy()\n",
    "            \n",
    "            # Make Detections\n",
    "            results = model(image) \n",
    "            \n",
    "            boxes = []\n",
    "            confidences = []\n",
    "            class_ids = []\n",
    "            \n",
    "            for result in results.xyxy[0]:\n",
    "                  class_id = result[-1]\n",
    "                  confidence = result[-2]\n",
    "                  if confidence > 0.7 and class_id == 0:\n",
    "                        center_x = int(result[0] * image.shape[1])\n",
    "                        center_y = int(result[1] * image.shape[0])\n",
    "                        width = int(result[2] * image.shape[1])\n",
    "                        height = int(result[3] * image.shape[0])\n",
    "                        left = int(center_x - width/2)\n",
    "                        top = int(center_y - height/2)\n",
    "                        boxes.append([left, top, width, height])\n",
    "                        confidences.append(float(confidence))\n",
    "                        class_ids.append(class_id)\n",
    "            \n",
    "            pose_tracker = mp.solutions.pose.Pose()\n",
    "            for i in range(len(boxes)):\n",
    "                  box = boxes[i]\n",
    "                  x, y, w, h = box\n",
    "                  cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                  crop = output_frame[y:y+h, x:x+w]\n",
    "                  \n",
    "                  # Run pose tracker.\n",
    "                  crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                  pose_result = pose_tracker.process(image=crop)\n",
    "                  pose_landmarks = pose_result.pose_landmarks\n",
    "                  \n",
    "                  if pose_landmarks is not None:\n",
    "                        mp.solutions.drawing_utils.draw_landmarks(output_frame[y:y+h, x:x+w], pose_result.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS,\n",
    "                                mp.solutions.drawing_utils.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp.solutions.drawing_utils.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )\n",
    "            \n",
    "            cv2.imshow('Result',output_frame)\n",
    "                  \n",
    "            if cv2.waitKey(1) & 0xFF == 27:\n",
    "                  break\n",
    "cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/julie3399/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-7-17 Python-3.11.1 torch-2.1.0.dev20230328 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "image 1/1: 720x1280 1 person, 1 cell phone\n",
      "Speed: 1.5ms pre-process, 81.7ms inference, 0.7ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([[7.19377e+02, 1.66656e+02, 9.88746e+02, 4.22906e+02, 8.74034e-01, 6.70000e+01],\n",
      "        [2.15004e+02, 1.26850e+02, 1.22454e+03, 7.12456e+02, 6.86175e-01, 0.00000e+00]])\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "image = cv2.imread('temp.jpg')\n",
    "result = model(image)\n",
    "result.print()\n",
    "#result.show()\n",
    "print('\\n', result.xyxy[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "class CentroidTracker:\n",
    "    def __init__(self, maxDisappeared=50, maxDistance=50):\n",
    "        # initialize the next unique object ID along with two ordered\n",
    "        # dictionaries used to keep track of mapping a given object\n",
    "        # ID to its centroid and number of consecutive frames it has\n",
    "        # been marked as \"disappeared\", respectively\n",
    "        self.nextObjectID = 0\n",
    "        self.objects = OrderedDict()\n",
    "        self.disappeared = OrderedDict()\n",
    "\n",
    "        # store the number of maximum consecutive frames a given\n",
    "        # object is allowed to be marked as \"disappeared\" until we\n",
    "        # need to deregister the object from tracking\n",
    "        self.maxDisappeared = maxDisappeared\n",
    "\n",
    "        # store the maximum distance between centroids to associate\n",
    "        # an object -- if the distance is larger than this maximum\n",
    "        # distance we'll start to mark the object as \"disappeared\"\n",
    "        self.maxDistance = maxDistance\n",
    "\n",
    "    def register(self, centroid):\n",
    "        # when registering an object we use the next available object\n",
    "        # ID to store the centroid\n",
    "        self.objects[self.nextObjectID] = centroid\n",
    "        self.disappeared[self.nextObjectID] = 0\n",
    "        self.nextObjectID += 1\n",
    "\n",
    "    def deregister(self, objectID):\n",
    "        # to deregister an object ID we delete the object ID from\n",
    "        # both of our respective dictionaries\n",
    "        del self.objects[objectID]\n",
    "        del self.disappeared[objectID]\n",
    "\n",
    "    def update(self, rects):\n",
    "        # check to see if the list of input bounding box rectangles\n",
    "        # is empty\n",
    "        if len(rects) == 0:\n",
    "            # loop over any existing tracked objects and mark them\n",
    "            # as disappeared\n",
    "            for objectID in list(self.disappeared.keys()):\n",
    "                self.disappeared[objectID] += 1\n",
    "\n",
    "        # if we have reached a maximum number of consecutive\n",
    "        # frames where a given object has been marked as\n",
    "        # missing, deregister it\n",
    "        if self.disappeared[objectID] > self.maxDisappeared:\n",
    "            self.deregister(objectID)\n",
    "            \n",
    "            # return early as there are no centroids or tracking info\n",
    "            # to update\n",
    "            return self.objects\n",
    "\n",
    "        # initialize an array of input centroids for the current frame\n",
    "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
    "\n",
    "        # loop over the bounding box rectangles\n",
    "        for (i, (startX, startY, endX, endY)) in enumerate(rects):\n",
    "            # use the bounding box coordinates to derive the centroid\n",
    "            cX = int((startX + endX) / 2.0)\n",
    "            cY = int((startY + endY) / 2.0)\n",
    "            inputCentroids[i] = (cX, cY)\n",
    "\n",
    "        # if we are currently not tracking any objects take the input\n",
    "        # centroids and register each of them\n",
    "        if len(self.objects) == 0:\n",
    "            for i in range(0, len(inputCentroids)):\n",
    "                self.register(inputCentroids[i])\n",
    "\n",
    "        # otherwise, are are currently tracking objects so we need to\n",
    "        # try to match the input centroids to existing object\n",
    "        # centroids\n",
    "        else:\n",
    "            # grab the set of object IDs and corresponding centroids\n",
    "            objectIDs = list(self.objects.keys())\n",
    "            objectCentroids = list(self.objects.values())\n",
    "\n",
    "            # compute the distance between each pair of object\n",
    "            # centroids and input centroids, respectively -- our\n",
    "            # goal will be to match an input centroid to an existing\n",
    "            # object centroid\n",
    "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
    "\n",
    "            # in order to perform this matching we must (1) find the\n",
    "            # smallest value in each row and then (2) sort the row\n",
    "            # indexes based on their minimum values so that the row\n",
    "            # with the smallest value as at the *front* of the index\n",
    "            # list\n",
    "            rows = D.min(axis=1).argsort()\n",
    "\n",
    "            # next, we perform a similar process on the columns by\n",
    "            # finding the smallest value in each column and then\n",
    "            # sorting using the previously computed row index list\n",
    "            cols = D.argmin(axis=1)[rows]\n",
    "\n",
    "            # in order to determine if we need to update, register,\n",
    "            # or deregister an object we need to keep track of which\n",
    "            # of the rows and column indexes we have already examined\n",
    "            usedRows = set()\n",
    "            usedCols = set()\n",
    "\n",
    "            # loop over the combination of the (row, column) index\n",
    "            # tuples\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                # if we have already examined either the row or\n",
    "                # column value before, ignore it\n",
    "                if row in usedRows or col in usedCols:\n",
    "                    continue\n",
    "\n",
    "                # if the distance between centroids is greater than\n",
    "                # the maximum distance, do not associate the two\n",
    "                # centroids to the same object\n",
    "                if D[row, col] > self.maxDistance:\n",
    "                    continue\n",
    "\n",
    "                # otherwise, grab the object ID for the current row,\n",
    "                # set its new centroid, and reset the disappeared\n",
    "                # counter\n",
    "                objectID = objectIDs[row]\n",
    "                self.objects[objectID] = inputCentroids[col]\n",
    "                self.disappeared[objectID] = 0\n",
    "\n",
    "                # indicate that we have examined each of the row and\n",
    "                # column indexes, respectively\n",
    "                usedRows.add(row)\n",
    "                usedCols.add(col)\n",
    "\n",
    "            # compute both the row and column index we have NOT yet\n",
    "            # examined\n",
    "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
    "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
    "\n",
    "            # in the event that the number of object centroids is\n",
    "            # equal or greater than the number of input centroids\n",
    "            # we need to check and see if some of these objects have\n",
    "            # potentially disappeared\n",
    "            if D.shape[0] >= D.shape[1]:\n",
    "                # loop over the unused row indexes\n",
    "                for row in unusedRows:\n",
    "                    # grab the object ID for the corresponding row\n",
    "                    # index and increment the disappeared counter\n",
    "                    objectID = objectIDs[row]\n",
    "                    self.disappeared[objectID] += 1\n",
    "\n",
    "                    # check to see if the number of consecutive\n",
    "                    # frames the object has been marked \"disappeared\"\n",
    "                    # for warrants deregistering the object\n",
    "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                        self.deregister(objectID)\n",
    "\n",
    "                    # otherwise, if the number of input centroids is greater\n",
    "                    # than the number of existing object centroids we need to\n",
    "                    # register each new input centroid as a trackable object\n",
    "                    else:\n",
    "                        for col in unusedCols:\n",
    "                            self.register(inputCentroids[col])\n",
    "\n",
    "            # return the set of trackable objects\n",
    "            return self.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: Attempt with one algorithm to solve the separation problem\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "# only pose estimation\n",
    "mp_pose = mp.solutions.pose\n",
    "# using holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "  \n",
    "pose =  mp_pose.Pose(min_detection_confidence=0.5,min_tracking_confidence=0.9)\n",
    "\n",
    "def process_image(image):\n",
    "    results = pose.process(image)\n",
    "    mp_drawing.draw_landmarks(\n",
    "    image,\n",
    "    results.pose_landmarks,\n",
    "    mp_pose.POSE_CONNECTIONS,\n",
    "    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "    return image\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def process_image_holistic(image):\n",
    "    results = holistic.process(image)\n",
    "    \n",
    "    # Draw face landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                                      mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(0,0,245), thickness=1, circle_radius=1)) # FACE_CONNECTIONS: draw the line between the points\n",
    "    # Right hand\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                      mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2))\n",
    "    # Left Hand\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "    # Pose Detections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(0,245,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/julie3399/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-7-17 Python-3.11.1 torch-2.1.0.dev20230328 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Load the YOLOv5 model\n",
    "yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Load the video\n",
    "# video = cv2.VideoCapture(0)\n",
    "video = cv2.VideoCapture(\"Multiple5.mp4\")\n",
    "\n",
    "# Get the video's width, height, and frames per second (fps)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "# Create a VideoWriter object to save the video\n",
    "output_file = 'output_video.mp4'  # Specify the output video file name\n",
    "video_writer = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "\n",
    "\n",
    "# Process each frame of the video\n",
    "while True:\n",
    "  # Read the next frame\n",
    "  success, frame = video.read()\n",
    "  if not success:\n",
    "    break\n",
    "\n",
    "  # Perform object detection on the frame\n",
    "  results = yolov5(frame)\n",
    "  detections = results.xyxy[0]\n",
    "  \n",
    "\n",
    "  # Check whether the bounding box centroids are inside the ROI\n",
    "  for i, detection in enumerate(detections):    \n",
    "    xmin    = detection[0]\n",
    "    ymin    = detection[1]\n",
    "    xmax    = detection[2]\n",
    "    ymax    = detection[3]\n",
    "    score   = detection[4]\n",
    "    class_id= detection[5]\n",
    "    centroid_x = int(xmin + xmax) // 2\n",
    "    centroid_y =  int(ymin + ymax) // 2\n",
    "\n",
    "    #Threshold score\n",
    "    if score >= 0.7: \n",
    "      if class_id == 0:\n",
    "        color = (255, 0, 0)\n",
    "        cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, 1)\n",
    "        cv2.putText(frame, f'Person {i}', (int(xmin), int(ymin)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "        #padding\n",
    "        #padding = 25\n",
    "        person = frame[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "\n",
    "        try:  ### mediapipe\n",
    "            # process_image(person)\n",
    "            process_image_holistic(person)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "      else:\n",
    "        pass\n",
    "\n",
    "\n",
    "  # Display the frame\n",
    "  cv2.imshow(\"Video\", frame)\n",
    "  video_writer.write(frame)\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "\n",
    "# Release the video capture object\n",
    "video.release()\n",
    "video_writer.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/julie3399/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-7-17 Python-3.11.1 torch-2.1.0.dev20230328 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Version 4: With tracking in\n",
    "\n",
    "# Add the Tracker to our Detection part\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Load the video\n",
    "video = cv2.VideoCapture(0)\n",
    "#video = cv2.VideoCapture(\"Multiple.mp4\")\n",
    "\n",
    "# Get the video's width, height, and frames per second (fps)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "# Create a VideoWriter object to save the video\n",
    "#output_file = 'output_video.mp4'  # Specify the output video file name\n",
    "#video_writer = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "# Process each frame of the video\n",
    "while True:\n",
    "  # Read the next frame\n",
    "  success, frame = video.read()\n",
    "  if not success:\n",
    "    break\n",
    "\n",
    "  # Perform object detection on the frame\n",
    "  results = yolov5(frame)\n",
    "  detections = results.xyxy[0]\n",
    "\n",
    "  boxes = []\n",
    "  class_ids = []\n",
    "  scores = []\n",
    "  \n",
    "  for i, detection in enumerate(detections):    \n",
    "    xmin    = detection[0]\n",
    "    ymin    = detection[1]\n",
    "    xmax    = detection[2]\n",
    "    ymax    = detection[3]\n",
    "    score   = detection[4]\n",
    "    class_id = detection[5]\n",
    "    centroid_x = int(xmin + xmax) // 2\n",
    "    centroid_y =  int(ymin + ymax) // 2\n",
    "    \n",
    "    if score > 0.7 and class_id == 0:\n",
    "        boxes.append([xmin,ymin,xmax-xmin,ymax-ymin])\n",
    "        class_ids.append(class_id)\n",
    "        scores.append(float(score))\n",
    "        \n",
    "        ### mediapipe\n",
    "        person = frame[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "        try:  \n",
    "            # process_image(person)\n",
    "            process_image_holistic(person)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # object tracking\n",
    "    #tracker = EuclideanDistTracker()\n",
    "    #boxes_ids = tracker.update(boxes)\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        centroid_x = x + w//2\n",
    "        centroid_y = y + h//2\n",
    "        (frame_h, frame_w) = frame.shape[:2]\n",
    "        \n",
    "        if centroid_x > frame_w//2:\n",
    "            id = 0\n",
    "        else:\n",
    "            id = 1\n",
    "        \n",
    "        cv2.putText(frame, str(id), (int(x), int(y) - 15), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 2)\n",
    "        cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 3)\n",
    "\n",
    "        \n",
    "  # Display the frame\n",
    "  cv2.imshow(\"Video\", frame)\n",
    "  video_writer.write(frame)\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "\n",
    "# Release the video capture object\n",
    "video.release()\n",
    "video_writer.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Dist Tracker (Not Used)\n",
    "import math\n",
    "\n",
    "# Define the Tracker\n",
    "class EuclideanDistTracker:\n",
    "    def __init__(self):\n",
    "        # Store the center positions of the objects\n",
    "        self.center_points = {}\n",
    "        # Keep the count of the IDs\n",
    "        # each time a new object id detected, the count will increase by one\n",
    "        self.id_count = 0\n",
    "\n",
    "\n",
    "    def update(self, objects_rect):\n",
    "        # Objects boxes and ids\n",
    "        objects_bbs_ids = []\n",
    "\n",
    "        # Get center point of new object\n",
    "        for rect in objects_rect:\n",
    "            x, y, w, h = rect\n",
    "            cx = (x + x + w) // 2\n",
    "            cy = (y + y + h) // 2\n",
    "\n",
    "            # Find out if that object was detected already\n",
    "            same_object_detected = False\n",
    "            for id, pt in self.center_points.items():\n",
    "                dist = math.hypot(cx - pt[0], cy - pt[1])\n",
    "\n",
    "                if dist < 25:\n",
    "                    self.center_points[id] = (cx, cy)\n",
    "                    print(self.center_points)\n",
    "                    objects_bbs_ids.append([x, y, w, h, id])\n",
    "                    same_object_detected = True\n",
    "                    break\n",
    "\n",
    "            # New object is detected we assign the ID to that object\n",
    "            if same_object_detected is False:\n",
    "                self.center_points[self.id_count] = (cx, cy)\n",
    "                objects_bbs_ids.append([x, y, w, h, self.id_count])\n",
    "                self.id_count += 1\n",
    "\n",
    "        # Clean the dictionary by center points to remove IDS not used anymore\n",
    "        new_center_points = {}\n",
    "        for obj_bb_id in objects_bbs_ids:\n",
    "            _, _, _, _, object_id = obj_bb_id\n",
    "            center = self.center_points[object_id]\n",
    "            new_center_points[object_id] = center\n",
    "\n",
    "        # Update dictionary with IDs not used removed\n",
    "        self.center_points = new_center_points.copy()\n",
    "        return objects_bbs_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 12) (3999349709.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    pose_estimator_dim.append(<detected object's boundary>)\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 12)\n"
     ]
    }
   ],
   "source": [
    "# Attempt 0: no progress\n",
    "def get_multi_poses(coordinates):\n",
    "   # global variables\n",
    "   pose_estimator = []\n",
    "   pose_estimator_dim = []\n",
    "   # For each object detected\n",
    "   # WHICH POSE ESTIMATOR TO USE.\n",
    "   selected_pose_idx=0\n",
    "                                                      \n",
    "   if(len(pose_estimator)==0): \n",
    "      pose = mp_pose.Pose(min_detection_confidence=0.6,     \n",
    "               min_tracking_confidence=0.6)\n",
    "      pose_estimator.append(pose)    \n",
    "      pose_estimator_dim.append(coordinates)\n",
    "      selected_pose_idx = len(pose_estimator)-1                            \n",
    "   elif(<object_id>>len(pose_estimator)):\n",
    "      thresholdForNew = 100\n",
    "      prev_high_score = 0\n",
    "      selected_pose_idx_high =0\n",
    "      prev_low_score = 1000000000\n",
    "      selected_pose_idx_low =0\n",
    "      pose_idx = 0\n",
    "      for dim in pose_estimator_dim:\n",
    "         score = compareDist(dim,coordinates)\n",
    "         if(score > prev_high_score):\n",
    "            selected_pose_idx_high  =  pose_idx\n",
    "            prev_high_score = score\n",
    "         if(score < prev_low_score):                                        \n",
    "            selected_pose_idx_low  =  pose_idx\n",
    "            prev_low_score = score\n",
    "         pose_idx+=1\n",
    "      if prev_high_score > thresholdForNew:\n",
    "         pose = mp_pose.Pose(min_detection_confidence=0.6,                   \n",
    "   min_tracking_confidence=0.6)\n",
    "         pose_estimator.append(pose)    \n",
    "         pose_estimator_dim.append(coordinates)\n",
    "         selected_pose_idx = len(pose_estimator)-1 \n",
    "      else:\n",
    "         selected_pose_idx = selected_pose_idx_low\n",
    "      pose_estimator_dim[selected_pose_idx]=coordinates\n",
    "                                       \n",
    "   else:\n",
    "      pose_idx = 0\n",
    "      prev_score = 1000000000                                \n",
    "      for dim in pose_estimator_dim:\n",
    "         score = compareDist(dim,[x_min, y_min, box_width, box_height])\n",
    "         if(score < prev_score):                                        \n",
    "            selected_pose_idx  =  pose_idx\n",
    "            prev_score = score   \n",
    "         pose_idx+=1\n",
    "      pose_estimator_dim[selected_pose_idx]=<detected object's boundary>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Object Detection by Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# from tracker import *\n",
    "\n",
    "# Create tracker object\n",
    "#tracker = EuclideanDistTracker()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Object detection from Stable camera\n",
    "object_detector = cv2.createBackgroundSubtractorMOG2(history=100,varThreshold=16,detectShadows = False)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Extract Region of interest\n",
    "\n",
    "    # 1. Object Detection\n",
    "    mask = object_detector.apply(frame)\n",
    "    _, mask = cv2.threshold(mask, 254, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        # Calculate area and remove small elements\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 3000:\n",
    "            #cv2.drawContours(roi, [cnt], -1, (0, 255, 0), 2)\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "\n",
    "   \n",
    "\n",
    "    cv2.imshow(\"roi\", frame)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Mask\", mask)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a ROI and then press SPACE or ENTER button!\n",
      "Cancel the selection process by pressing c button!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 09:44:21.380 Python[56727:6169926] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-14 09:44:21.380 Python[56727:6169926] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-14 09:44:21.380 Python[56727:6169926] Text input context does not respond to _valueForTIProperty:\n",
      "2023-07-14 09:44:21.380 Python[56727:6169926] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-14 09:44:21.380 Python[56727:6169926] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-14 09:44:21.380 Python[56727:6169926] Text input context does not respond to _valueForTIProperty:\n",
      "2023-07-14 09:44:23.682 Python[56727:6169926] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-14 09:44:23.682 Python[56727:6169926] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-14 09:44:23.682 Python[56727:6169926] Text input context does not respond to _valueForTIProperty:\n",
      "2023-07-14 09:44:23.693 Python[56727:6169926] _TIPropertyValueIsValid called with 4 on nil context!\n",
      "2023-07-14 09:44:23.693 Python[56727:6169926] imkxpc_getApplicationProperty:reply: called with incorrect property value 4, bailing.\n",
      "2023-07-14 09:44:23.693 Python[56727:6169926] Text input context does not respond to _valueForTIProperty:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m bbox \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m287\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m86\u001b[39m, \u001b[38;5;241m320\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Uncomment the line below to select a different bounding box\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m bbox \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectROI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Initialize tracker with first frame and bounding box\u001b[39;00m\n\u001b[1;32m     25\u001b[0m ok \u001b[38;5;241m=\u001b[39m tracker\u001b[38;5;241m.\u001b[39minit(frame, bbox)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tracker_type = 'MEDIANFLOW'\n",
    "if tracker_type == 'MEDIANFLOW':\n",
    "    tracker = cv2.TrackerCSRT_create()\n",
    "\n",
    " \n",
    "    # Read video\n",
    "    video = cv2.VideoCapture(0)\n",
    " \n",
    "    # Exit if video not opened.\n",
    "    if not video.isOpened():\n",
    "        print(\"Could not open video\")\n",
    " \n",
    "    # Read first frame.\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        print('Cannot read video file')\n",
    "     \n",
    "    # Define an initial bounding box\n",
    "    bbox = (287, 23, 86, 320)\n",
    " \n",
    "    # Uncomment the line below to select a different bounding box\n",
    "    bbox = cv2.selectROI(frame, False)\n",
    " \n",
    "    # Initialize tracker with first frame and bounding box\n",
    "    ok = tracker.init(frame, bbox)\n",
    " \n",
    "    while True:\n",
    "        # Read a new frame\n",
    "        ok, frame = video.read()\n",
    "        if not ok:\n",
    "            break\n",
    "         \n",
    "        # Start timer\n",
    "        timer = cv2.getTickCount()\n",
    " \n",
    "        # Update tracker\n",
    "        ok, bbox = tracker.update(frame)\n",
    " \n",
    "        # Calculate Frames per second (FPS)\n",
    "        fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer);\n",
    " \n",
    "        # Draw bounding box\n",
    "        if ok:\n",
    "            # Tracking success\n",
    "            p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "        else :\n",
    "            # Tracking failure\n",
    "            cv2.putText(frame, \"Tracking failure detected\", (100,80), cv2.FONT_HERSHEY_SIMPLEX, 0.75,(0,0,255),2)\n",
    " \n",
    "        # Display tracker type on frame\n",
    "        cv2.putText(frame, tracker_type + \" Tracker\", (100,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2);\n",
    "     \n",
    "        # Display FPS on frame\n",
    "        cv2.putText(frame, \"FPS : \" + str(int(fps)), (100,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50), 2);\n",
    " \n",
    "        # Display result\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    " \n",
    "        # Exit if ESC pressed\n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "        if k == 27 : break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting schedule\n",
      "  Downloading schedule-1.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "screenshot taken\n",
      "closing the app\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'cv2.VideoCapture' object has no attribute 'destroyAllWindows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     38\u001b[0m cam\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mcam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroyAllWindows\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'cv2.VideoCapture' object has no attribute 'destroyAllWindows'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import schedule\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"Webcam\")\n",
    "img_counter = 0\n",
    "\n",
    "\n",
    "def capture():\n",
    "    global img_counter\n",
    "    img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "    cv2.imwrite(img_name, frame)\n",
    "    print(\"screenshot taken\")\n",
    "    img_counter += 1\n",
    "\n",
    "\n",
    "# Set up schedule before loop\n",
    "\n",
    "schedule.every(2).seconds.do(capture)\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"test\", frame)\n",
    "    schedule.run_pending()\n",
    "\n",
    "    k = cv2.waitKey(100)  # 1/10 sec delay; no need for separate sleep\n",
    "\n",
    "    if k % 256 == 27:\n",
    "        print(\"closing the app\")\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cam.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: ç›®æ ‡è·¯å¾„ 'darknet' å·²ç»å­˜åœ¨ï¼Œå¹¶ä¸”ä¸æ˜¯ä¸€ä¸ªç©ºç›®å½•ã€‚\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pjreddie/darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: *** No targets specified and no makefile found.  Stop.\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-17 15:30:13--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: 'yolov3.weights'\n",
      "\n",
      "yolov3.weights      100%[===================>] 236.52M  10.3MB/s    in 24s     \n",
      "\n",
      "2023-07-17 15:30:38 (9.88 MB/s) - 'yolov3.weights' saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMADictSmoothing(object):\n",
    "  \"\"\"Smoothes pose classification.\"\"\"\n",
    "\n",
    "  def __init__(self, window_size=10, alpha=0.2):\n",
    "    self._window_size = window_size\n",
    "    self._alpha = alpha\n",
    "\n",
    "    self._data_in_window = []\n",
    "\n",
    "  def __call__(self, data):\n",
    "    \"\"\"Smoothes given pose classification.\n",
    "\n",
    "    Smoothing is done by computing Exponential Moving Average for every pose\n",
    "    class observed in the given time window. Missed pose classes arre replaced\n",
    "    with 0.\n",
    "    \n",
    "    Args:\n",
    "      data: Dictionary with pose classification. Sample:\n",
    "          {\n",
    "            'pushups_down': 8,\n",
    "            'pushups_up': 2,\n",
    "          }\n",
    "\n",
    "    Result:\n",
    "      Dictionary in the same format but with smoothed and float instead of\n",
    "      integer values. Sample:\n",
    "        {\n",
    "          'pushups_down': 8.3,\n",
    "          'pushups_up': 1.7,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Add new data to the beginning of the window for simpler code.\n",
    "    self._data_in_window.insert(0, data)\n",
    "    self._data_in_window = self._data_in_window[:self._window_size]\n",
    "\n",
    "    # Get all keys.\n",
    "    keys = set([key for data in self._data_in_window for key, _ in data.items()])\n",
    "\n",
    "    # Get smoothed values.\n",
    "    smoothed_data = dict()\n",
    "    for key in keys:\n",
    "      factor = 1.0\n",
    "      top_sum = 0.0\n",
    "      bottom_sum = 0.0\n",
    "      for data in self._data_in_window:\n",
    "        value = data[key] if key in data else 0.0\n",
    "\n",
    "        top_sum += factor * value\n",
    "        bottom_sum += factor\n",
    "\n",
    "        # Update factor.\n",
    "        factor *= (1.0 - self._alpha)\n",
    "\n",
    "      smoothed_data[key] = top_sum / bottom_sum\n",
    "\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseClassifier(object):\n",
    "  \"\"\"Classifies pose landmarks.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               pose_samples_folder,\n",
    "               pose_embedder,\n",
    "               file_extension='csv',\n",
    "               file_separator=',',\n",
    "               n_landmarks=33,\n",
    "               n_dimensions=3,\n",
    "               top_n_by_max_distance=30,\n",
    "               top_n_by_mean_distance=10,\n",
    "               axes_weights=(1., 1., 0.2)):\n",
    "    self._pose_embedder = pose_embedder\n",
    "    self._n_landmarks = n_landmarks\n",
    "    self._n_dimensions = n_dimensions\n",
    "    self._top_n_by_max_distance = top_n_by_max_distance\n",
    "    self._top_n_by_mean_distance = top_n_by_mean_distance\n",
    "    self._axes_weights = axes_weights\n",
    "\n",
    "    self._pose_samples = self._load_pose_samples(pose_samples_folder,\n",
    "                                                 file_extension,\n",
    "                                                 file_separator,\n",
    "                                                 n_landmarks,\n",
    "                                                 n_dimensions,\n",
    "                                                 pose_embedder)\n",
    "\n",
    "  def _load_pose_samples(self,\n",
    "                         pose_samples_folder,\n",
    "                         file_extension,\n",
    "                         file_separator,\n",
    "                         n_landmarks,\n",
    "                         n_dimensions,\n",
    "                         pose_embedder):\n",
    "    \"\"\"Loads pose samples from a given folder.\n",
    "    \n",
    "    Required folder structure:\n",
    "      neutral_standing.csv\n",
    "      pushups_down.csv\n",
    "      pushups_up.csv\n",
    "      squats_down.csv\n",
    "      ...\n",
    "\n",
    "    Required CSV structure:\n",
    "      sample_00001,x1,y1,z1,x2,y2,z2,....\n",
    "      sample_00002,x1,y1,z1,x2,y2,z2,....\n",
    "      ...\n",
    "    \"\"\"\n",
    "    # Each file in the folder represents one pose class.\n",
    "    file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
    "\n",
    "    pose_samples = []\n",
    "    for file_name in file_names:\n",
    "      # Use file name as pose class name.\n",
    "      class_name = file_name[:-(len(file_extension) + 1)]\n",
    "      \n",
    "      # Parse CSV.\n",
    "      with open(os.path.join(pose_samples_folder, file_name)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=file_separator)\n",
    "        for row in csv_reader:\n",
    "          assert len(row) == n_landmarks * n_dimensions + 1, 'Wrong number of values: {}'.format(len(row))\n",
    "          landmarks = np.array(row[1:], np.float32).reshape([n_landmarks, n_dimensions])\n",
    "          pose_samples.append(PoseSample(\n",
    "              name=row[0],\n",
    "              landmarks=landmarks,\n",
    "              class_name=class_name,\n",
    "              embedding=pose_embedder(landmarks),\n",
    "          ))\n",
    "\n",
    "    return pose_samples\n",
    "\n",
    "  def find_pose_sample_outliers(self):\n",
    "    \"\"\"Classifies each sample against the entire database.\"\"\"\n",
    "    # Find outliers in target poses\n",
    "    outliers = []\n",
    "    for sample in self._pose_samples:\n",
    "      # Find nearest poses for the target one.\n",
    "      pose_landmarks = sample.landmarks.copy()\n",
    "      pose_classification = self.__call__(pose_landmarks)\n",
    "      class_names = [class_name for class_name, count in pose_classification.items() if count == max(pose_classification.values())]\n",
    "\n",
    "      # Sample is an outlier if nearest poses have different class or more than\n",
    "      # one pose class is detected as nearest.\n",
    "      if sample.class_name not in class_names or len(class_names) != 1:\n",
    "        outliers.append(PoseSampleOutlier(sample, class_names, pose_classification))\n",
    "\n",
    "    return outliers\n",
    "\n",
    "  def __call__(self, pose_landmarks):\n",
    "    \"\"\"Classifies given pose.\n",
    "\n",
    "    Classification is done in two stages:\n",
    "      * First we pick top-N samples by MAX distance. It allows to remove samples\n",
    "        that are almost the same as given pose, but has few joints bent in the\n",
    "        other direction.\n",
    "      * Then we pick top-N samples by MEAN distance. After outliers are removed\n",
    "        on a previous step, we can pick samples that are closes on average.\n",
    "    \n",
    "    Args:\n",
    "      pose_landmarks: NumPy array with 3D landmarks of shape (N, 3).\n",
    "\n",
    "    Returns:\n",
    "      Dictionary with count of nearest pose samples from the database. Sample:\n",
    "        {\n",
    "          'pushups_down': 8,\n",
    "          'pushups_up': 2,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Check that provided and target poses have the same shape.\n",
    "    assert pose_landmarks.shape == (self._n_landmarks, self._n_dimensions), 'Unexpected shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "    # Get given pose embedding.\n",
    "    pose_embedding = self._pose_embedder(pose_landmarks)\n",
    "    flipped_pose_embedding = self._pose_embedder(pose_landmarks * np.array([-1, 1, 1]))\n",
    "\n",
    "    # Filter by max distance.\n",
    "    #\n",
    "    # That helps to remove outliers - poses that are almost the same as the\n",
    "    # given one, but has one joint bent into another direction and actually\n",
    "    # represnt a different pose class.\n",
    "    max_dist_heap = []\n",
    "    for sample_idx, sample in enumerate(self._pose_samples):\n",
    "      max_dist = min(\n",
    "          np.max(np.abs(sample.embedding - pose_embedding) * self._axes_weights),\n",
    "          np.max(np.abs(sample.embedding - flipped_pose_embedding) * self._axes_weights),\n",
    "      )\n",
    "      max_dist_heap.append([max_dist, sample_idx])\n",
    "\n",
    "    max_dist_heap = sorted(max_dist_heap, key=lambda x: x[0])\n",
    "    max_dist_heap = max_dist_heap[:self._top_n_by_max_distance]\n",
    "\n",
    "    # Filter by mean distance.\n",
    "    #\n",
    "    # After removing outliers we can find the nearest pose by mean distance.\n",
    "    mean_dist_heap = []\n",
    "    for _, sample_idx in max_dist_heap:\n",
    "      sample = self._pose_samples[sample_idx]\n",
    "      mean_dist = min(\n",
    "          np.mean(np.abs(sample.embedding - pose_embedding) * self._axes_weights),\n",
    "          np.mean(np.abs(sample.embedding - flipped_pose_embedding) * self._axes_weights),\n",
    "      )\n",
    "      mean_dist_heap.append([mean_dist, sample_idx])\n",
    "\n",
    "    mean_dist_heap = sorted(mean_dist_heap, key=lambda x: x[0])\n",
    "    mean_dist_heap = mean_dist_heap[:self._top_n_by_mean_distance]\n",
    "\n",
    "    # Collect results into map: (class_name -> n_samples)\n",
    "    class_names = [self._pose_samples[sample_idx].class_name for _, sample_idx in mean_dist_heap]\n",
    "    result = {class_name: class_names.count(class_name) for class_name in set(class_names)}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA) \n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = mp.solutions.pose.Pose()\n",
    "nms_threshold = 0.5\n",
    "video_cap = cv2.VideoCapture(0)\n",
    "pose_tracker = mp_pose.Pose(upper_body_only=False)\n",
    "pose_classification_filter = EMADictSmoothing( window_size=10, alpha=0.2)\n",
    "pose_samples_folder = '/content/drive/MyDrive/BTP/Database/fitness_poses_csvs_out'\n",
    "pose_classifier = PoseClassifier( pose_samples_folder=pose_samples_folder, pose_embedder=pose_embedder, top_n_by_max_distance=30, top_n_by_mean_distance=10)\n",
    "repetition_counter = RepetitionCounter( class_name=class_name, enter_threshold=6, exit_threshold=4)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    success, frame = video_cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Perform object detection using YOLOv3\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence >0.7 and class_id == 0:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                width = int(detection[2] * frame.shape[1])\n",
    "                height = int(detection[3] * frame.shape[0])\n",
    "                left = int(center_x - width/2)\n",
    "                top = int(center_y - height/2)\n",
    "                boxes.append([left, top, width, height])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maximum suppression to the boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.96, nms_threshold)    \n",
    "    # Perform pose detection using Mediapipe\n",
    "    print(indices)\n",
    "    for i in indices:\n",
    "        print(i)\n",
    "\n",
    "        box = boxes[i[0]]\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        crop = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Convert the cropped image to RGB and feed it to the pose detection model\n",
    "        crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        crop_rgb.flags.writeable = False\n",
    "        results = pose.process(crop_rgb)\n",
    "        crop_rgb.flags.writeable = True\n",
    "        pose_landmarks = results.pose_landmarks\n",
    "        crop_rgb = cv2.cvtColor(crop_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Draw the pose landmarks on the frame\n",
    "        mp_drawing.draw_landmarks(crop, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                )\n",
    "        if pose_landmarks is not None:\n",
    "            frame_height, frame_width = crop.shape[0], crop.shape[1]\n",
    "            pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                                    for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "            assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "            # Classify the pose on the current frame.\n",
    "            pose_classification = pose_classifier(pose_landmarks)\n",
    "\n",
    "            # Smooth classification using EMA.\n",
    "            pose_classification_filtered = pose_classification_filter(pose_classification)\n",
    "\n",
    "            # Count repetitions.\n",
    "            repetitions_count = repetition_counter(pose_classification_filtered)\n",
    "\n",
    "        else:\n",
    "            # No pose => no classification on current frame.\n",
    "            pose_classification = None\n",
    "\n",
    "            # Still add empty classification to the filter to maintaing correct\n",
    "            # smoothing for future frames.\n",
    "            pose_classification_filtered = pose_classification_filter(dict())\n",
    "            pose_classification_filtered = None\n",
    "\n",
    "            # Don't update the counter presuming that person is 'frozen'. Just\n",
    "            # take the latest repetitions count.\n",
    "            repetitions_count = repetition_counter.n_repeats\n",
    "\n",
    "\n",
    "    cv2.putText(frame, str(repetitions_count), \n",
    "            (10,60), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)        \n",
    "\n",
    "    # Display the resulting frame\n",
    "\n",
    "    cv2.imshow(frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@6.899] global net_impl.cpp:178 setUpNet DNN module was not built with CUDA backend; switching to CPU\n",
      "[ERROR:0@6.904] global net_impl.cpp:1169 getLayerShapesRecursively OPENCV/DNN: [Convolution]:(conv_0): getMemoryShapes() throws exception. inputs=1 outputs=0/1 blobs=1\n",
      "[ERROR:0@6.904] global net_impl.cpp:1172 getLayerShapesRecursively     input[0] = [ 1 1 416 416 ]\n",
      "[ERROR:0@6.904] global net_impl.cpp:1180 getLayerShapesRecursively     blobs[0] = CV_32FC1 [ 32 3 3 3 ]\n",
      "[ERROR:0@6.904] global net_impl.cpp:1182 getLayerShapesRecursively Exception message: OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/layers/convolution_layer.cpp:417: error: (-2:Unspecified error) Number of input channels should be multiple of 3 but got 1 in function 'getMemoryShapes'\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/layers/convolution_layer.cpp:417: error: (-2:Unspecified error) Number of input channels should be multiple of 3 but got 1 in function 'getMemoryShapes'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m blob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(input_frame, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m, (\u001b[38;5;241m416\u001b[39m, \u001b[38;5;241m416\u001b[39m), swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(blob)\n\u001b[0;32m---> 43\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetUnconnectedOutLayersNames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m confidences \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/layers/convolution_layer.cpp:417: error: (-2:Unspecified error) Number of input channels should be multiple of 3 but got 1 in function 'getMemoryShapes'\n"
     ]
    }
   ],
   "source": [
    "# Run classification on a video(was working with single person video).\n",
    "import os\n",
    "import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "# Open output video.\n",
    "video_cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load YOLOv3 model\n",
    "net = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# Initialize variables for graph\n",
    "frame_count = 0\n",
    "total_time = 0\n",
    "processing_times = []\n",
    "\n",
    "# Define the NMS threshold\n",
    "nms_threshold = 0.5\n",
    "\n",
    "frame_idx = 0\n",
    "output_frame = None\n",
    "\n",
    "while True:\n",
    "    # Get next frame of the video.\n",
    "    success, input_frame = True, cv2.imread('temp.jpg', 0) \n",
    "    if not success:\n",
    "      break\n",
    "    #starting time for graph\n",
    "    start_time = time.time()\n",
    "    \n",
    "    output_frame = input_frame.copy()\n",
    "    # Perform object detection using YOLOv3\n",
    "    blob = cv2.dnn.blobFromImage(input_frame, 1/255, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.7 and class_id == 0:\n",
    "                center_x = int(detection[0] * input_frame.shape[1])\n",
    "                center_y = int(detection[1] * input_frame.shape[0])\n",
    "                width = int(detection[2] * input_frame.shape[1])\n",
    "                height = int(detection[3] * input_frame.shape[0])\n",
    "                left = int(center_x - width/2)\n",
    "                top = int(center_y - height/2)\n",
    "                boxes.append([left, top, width, height])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    # Apply non-maximum suppression to the boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.96, nms_threshold)    \n",
    "    \n",
    "    pose_tracker = mp_pose.Pose()\n",
    "    #pose_classifier = PoseClassifier(pose_samples_folder=pose_samples_folder,pose_embedder=pose_embedder,top_n_by_max_distance=35,#30top_n_by_mean_distance=10)#10\n",
    "    \n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        crop = output_frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Run pose tracker.\n",
    "        crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "        result = pose_tracker.process(image=crop)\n",
    "        pose_landmarks = result.pose_landmarks\n",
    "        # Draw pose prediction.\n",
    "        \n",
    "        if pose_landmarks is not None:\n",
    "            mp_drawing.draw_landmarks(output_frame[y:y+h, x:x+w], result.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )\n",
    "        if pose_landmarks is not None:\n",
    "          # # Get landmarks.\n",
    "            frame_height, frame_width = crop.shape[0], crop.shape[1]\n",
    "            pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                                 for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "            assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "\n",
    "        else:\n",
    "        # No pose => no classification on current frame.\n",
    "          pose_classification = None\n",
    "\n",
    "      # Still add empty classification to the filter to maintaing correct\n",
    "      # smoothing for future frames.\n",
    "          #pose_classification_filtered = pose_classification_filter(dict())\n",
    "          pose_classification_filtered = None\n",
    "\n",
    "    \n",
    "    #ending time for each frame\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    # Accumulate the frame processing time\n",
    "    total_time += processing_time\n",
    "    frame_count += 1\n",
    "     # Store the processing time for plotting\n",
    "    processing_times.append(processing_time)\n",
    "    # Create x-axis values for frame numbers\n",
    "    frame_numbers = np.arange(1, len(processing_times)+1)\n",
    "    \n",
    "    # Save the output frame.\n",
    "    output_frame = cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Show intermediate frames of the video to track progress.\n",
    "    cv2.imshow('Output',output_frame)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the average frame processing time\n",
    "average_time = total_time / frame_count\n",
    "print(f\"Average processing time: {average_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
