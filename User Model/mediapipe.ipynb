{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For single person:\n",
    "\n",
    "Codes adapted from https://github.com/nicknochnack/Full-Body-Estimation-using-Media-Pipe-Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        \n",
    "        if success:\n",
    "            # Recolor Feed (because mediapipe uses RGB while cv2 uses BGR)\n",
    "            #image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # Make Detections\n",
    "            results = holistic.process(image)\n",
    "            # print(results.face_landmarks)  # this will just be numbers/coordinates\n",
    "            \n",
    "            # the four landmarks we have:\n",
    "            # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "            \n",
    "            # Recolor image back to BGR for rendering, because cv2 loves BGR\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            \n",
    "            # Draw face landmarks\n",
    "            mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                                      mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(0,0,245), thickness=1, circle_radius=1)) # FACE_CONNECTIONS: draw the line between the points\n",
    "            # Right hand\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                      mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2))\n",
    "            # Left Hand\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "            # Pose Detections\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(0,245,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "                        \n",
    "        \n",
    "            cv2.imshow('Result from Holistic Model', image)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For two people:\n",
    "\n",
    "Idea from https://shawntng.medium.com/multi-person-pose-estimation-with-mediapipe-52e6a60839dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "- Use YOLO to detect each person and crop\n",
    "- Use Mediapipe for single person detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/julie3399/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-8-28 Python-3.11.4 torch-2.0.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Load the YOLOv5 model\n",
    "yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Load the video\n",
    "#video = cv2.VideoCapture('Multiple4.MP4')\n",
    "video = cv2.VideoCapture(\"Multiple5.mp4\")\n",
    "\n",
    "# Get the video's width, height, and frames per second (fps)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Create a VideoWriter object to save the video\n",
    "output_file = 'output_video.mp4'  # Specify the output video file name\n",
    "video_writer = cv2.VideoWriter(output_file, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "  \n",
    "\n",
    "# def process_image(image):\n",
    "#     results = pose.process(image)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#     image,\n",
    "#     results.pose_landmarks,\n",
    "#     mp_pose.POSE_CONNECTIONS,\n",
    "#     landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "#     return image\n",
    "\n",
    "\n",
    "def process_image_holistic(image):\n",
    "    results = holistic.process(image)\n",
    "    \n",
    "    # # Draw face landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                                      mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),\n",
    "                                        mp_drawing.DrawingSpec(color=(0,0,245), thickness=1, circle_radius=1)) # FACE_CONNECTIONS: draw the line between the points\n",
    "    # # Right hand\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                      mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                      mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2))\n",
    "    # # Left Hand\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(255,0,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,0,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "    # Pose Detections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(0,255,0), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(0,245,0), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# Process each frame of the video\n",
    "while True:\n",
    "  # Read the next frame\n",
    "  success, frame = video.read()\n",
    "  if not success:\n",
    "    break\n",
    "\n",
    "  # Perform object detection on the frame\n",
    "  results = yolov5(frame)\n",
    "  detections = results.xyxy[0]\n",
    "\n",
    "  boxes = []\n",
    "  class_ids = []\n",
    "  scores = []\n",
    "  \n",
    "  for i, detection in enumerate(detections):    \n",
    "    xmin    = detection[0]\n",
    "    ymin    = detection[1]\n",
    "    xmax    = detection[2]\n",
    "    ymax    = detection[3]\n",
    "    score   = detection[4]\n",
    "    class_id = detection[5]\n",
    "    centroid_x = int(xmin + xmax) // 2\n",
    "    centroid_y =  int(ymin + ymax) // 2\n",
    "    \n",
    "    if score > 0.7 and class_id == 0:\n",
    "        boxes.append([xmin,ymin,xmax-xmin,ymax-ymin])\n",
    "        class_ids.append(class_id)\n",
    "        scores.append(float(score))\n",
    "        \n",
    "        ### mediapipe\n",
    "        person = frame[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
    "        try:  \n",
    "            # process_image(person)\n",
    "            process_image_holistic(person)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # object tracking\n",
    "    #tracker = EuclideanDistTracker()\n",
    "    #boxes_ids = tracker.update(boxes)\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        centroid_x = x + w//2\n",
    "        centroid_y = y + h//2\n",
    "        (frame_h, frame_w) = frame.shape[:2]\n",
    "        \n",
    "        if centroid_x > frame_w//2:\n",
    "            id = 0\n",
    "        else:\n",
    "            id = 1\n",
    "        \n",
    "        cv2.putText(frame, f'Person {id}', (int(x), int(y) - 15), cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 2)\n",
    "        cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 3)\n",
    "\n",
    "        \n",
    "  # Display the frame\n",
    "  cv2.imshow(\"Video\", frame)\n",
    "  video_writer.write(frame)\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "\n",
    "# Release the video capture object\n",
    "video.release()\n",
    "video_writer.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
