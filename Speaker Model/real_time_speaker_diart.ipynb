{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_speakers = 2 #@param {type:\"integer\"}\n",
    "\n",
    "language = 'English' #@param ['any', 'English']\n",
    "\n",
    "model_size = 'large' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
    "\n",
    "\n",
    "model_name = model_size\n",
    "if language == 'English' and model_size != 'large':\n",
    "  model_name += '.en'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit\n",
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import datetime\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import pyannote.audio\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "embedding_model = PretrainedSpeakerEmbedding( \n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "from pyannote.core import Segment\n",
    "\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_speakers = 2 #@param {type:\"integer\"}\n",
    "\n",
    "language = 'English' #@param ['any', 'English']\n",
    "\n",
    "model_size = 'tiny' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
    "\n",
    "model_name = model_size\n",
    "if language == 'English' and model_size != 'large':\n",
    "  model_name += '.en'\n",
    "\n",
    "model = whisper.load_model(model_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'IMG_9976.wav'\n",
    "\n",
    "result = model.transcribe(path)\n",
    "segments = result[\"segments\"]\n",
    "with contextlib.closing(wave.open(path,'r')) as f:\n",
    "  frames = f.getnframes()\n",
    "  rate = f.getframerate()\n",
    "  duration = frames / float(rate)\n",
    "  \n",
    "audio = Audio()\n",
    "\n",
    "def segment_embedding(segment):\n",
    "  start = segment[\"start\"]\n",
    "  # Whisper overshoots the end timestamp in the last segment\n",
    "  end = min(duration, segment[\"end\"])\n",
    "  clip = Segment(start, end)\n",
    "  waveform, sample_rate = audio.crop(path, clip)\n",
    "  return embedding_model(waveform[None])\n",
    "embeddings = np.zeros(shape=(len(segments), 192))\n",
    "for i, segment in enumerate(segments):\n",
    "  embeddings[i] = segment_embedding(segment)\n",
    "\n",
    "embeddings = np.nan_to_num(embeddings)\n",
    "\n",
    "\n",
    "clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
    "labels = clustering.labels_\n",
    "for i in range(len(segments)):\n",
    "  segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
    "\n",
    "\n",
    "def time(secs):\n",
    "      return datetime.timedelta(seconds=round(secs))\n",
    "\n",
    "f = open(\"transcript.txt\", \"w\")\n",
    "\n",
    "for (i, segment) in enumerate(segments):\n",
    "  if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "    f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
    "  f.write(segment[\"text\"][1:] + ' ')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_audio(record_sec):\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 16000\n",
    "    CHUNK = 1024\n",
    "    RECORD_SECONDS = record_sec\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    stream = audio.open(\n",
    "        format=FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK,\n",
    "        input_device_index=0\n",
    "    )\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    waveFile.setnchannels(CHANNELS)\n",
    "    waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    waveFile.setframerate(RATE)\n",
    "    waveFile.writeframes(b''.join(frames))\n",
    "    waveFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" for this toy here yeah it's hot every connected like this oh here's the magnet oh okay and we can choose something like this\",\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 2.0,\n",
       "   'text': \" for this toy here yeah it's hot\",\n",
       "   'tokens': [50364, 337, 341, 12058, 510, 1338, 309, 311, 2368, 50464],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.7810148806185335,\n",
       "   'compression_ratio': 1.3333333333333333,\n",
       "   'no_speech_prob': 0.09438203275203705,\n",
       "   'speaker': 'SPEAKER 1'},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 4.0,\n",
       "   'end': 6.0,\n",
       "   'text': ' every connected like this',\n",
       "   'tokens': [50564, 633, 4582, 411, 341, 50664],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.7810148806185335,\n",
       "   'compression_ratio': 1.3333333333333333,\n",
       "   'no_speech_prob': 0.09438203275203705,\n",
       "   'speaker': 'SPEAKER 1'},\n",
       "  {'id': 2,\n",
       "   'seek': 0,\n",
       "   'start': 6.0,\n",
       "   'end': 8.0,\n",
       "   'text': \" oh here's the magnet\",\n",
       "   'tokens': [50664, 1954, 510, 311, 264, 15211, 50764],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.7810148806185335,\n",
       "   'compression_ratio': 1.3333333333333333,\n",
       "   'no_speech_prob': 0.09438203275203705,\n",
       "   'speaker': 'SPEAKER 2'},\n",
       "  {'id': 3,\n",
       "   'seek': 0,\n",
       "   'start': 8.0,\n",
       "   'end': 10.0,\n",
       "   'text': ' oh okay',\n",
       "   'tokens': [50764, 1954, 1392, 50864],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.7810148806185335,\n",
       "   'compression_ratio': 1.3333333333333333,\n",
       "   'no_speech_prob': 0.09438203275203705,\n",
       "   'speaker': 'SPEAKER 1'},\n",
       "  {'id': 4,\n",
       "   'seek': 0,\n",
       "   'start': 12.0,\n",
       "   'end': 16.0,\n",
       "   'text': ' and we can choose something like this',\n",
       "   'tokens': [50964, 293, 321, 393, 2826, 746, 411, 341, 51164],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.7810148806185335,\n",
       "   'compression_ratio': 1.3333333333333333,\n",
       "   'no_speech_prob': 0.09438203275203705,\n",
       "   'speaker': 'SPEAKER 1'}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"transcript.txt\", \"w\")\n",
    "f.write(\"Speaker Diarization\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_audio()\n",
    "\n",
    "path = 'output.wav'\n",
    "\n",
    "result = model.transcribe(path)\n",
    "segments = result[\"segments\"]\n",
    "with contextlib.closing(wave.open(path,'r')) as f:\n",
    "  frames = f.getnframes()\n",
    "  rate = f.getframerate()\n",
    "  duration = frames / float(rate)\n",
    "  \n",
    "audio = Audio()\n",
    "\n",
    "def segment_embedding(segment):\n",
    "  start = segment[\"start\"]\n",
    "  # Whisper overshoots the end timestamp in the last segment\n",
    "  end = min(duration, segment[\"end\"])\n",
    "  clip = Segment(start, end)\n",
    "  waveform, sample_rate = audio.crop(path, clip)\n",
    "  return embedding_model(waveform[None])\n",
    "embeddings = np.zeros(shape=(len(segments), 192))\n",
    "for i, segment in enumerate(segments):\n",
    "  embeddings[i] = segment_embedding(segment)\n",
    "\n",
    "embeddings = np.nan_to_num(embeddings)\n",
    "\n",
    "\n",
    "clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
    "labels = clustering.labels_\n",
    "for i in range(len(segments)):\n",
    "  segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
    "\n",
    "\n",
    "def time(secs):\n",
    "      return datetime.timedelta(seconds=round(secs))\n",
    "\n",
    "f = open(\"transcript.txt\", \"a\")\n",
    "\n",
    "for (i, segment) in enumerate(segments):\n",
    "  if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
    "    f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
    "  f.write(segment[\"text\"][1:] + ' ')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
