{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw,ImageFont\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatAngle(boardBW):\n",
    "    \n",
    "\tcoords = np.column_stack(np.where(boardBW > 0))\n",
    "\tangle = cv2.minAreaRect(coords)[-1]\n",
    "\n",
    "\tif angle < -45:\n",
    "\t\tangle = -(90 + angle)\n",
    "\t \n",
    "\telse:\n",
    "\t\tangle = -angle\n",
    "\t\t\n",
    "\treturn angle\n",
    "\n",
    "\n",
    "def tilt(image, angle):\n",
    "    \t# rotate the image to deskew it\n",
    "\t(h, w) = image.shape[:2]\n",
    "\tcenter = (w // 2, h // 2)\n",
    "\tM = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\trotated = cv2.warpAffine(image, M, (w, h),\n",
    "\tflags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "\treturn rotated\n",
    "\n",
    "# def whatAngle(boardBW):\n",
    "#     coords = np.column_stack(np.where(boardBW > 0))\n",
    "#     rect = cv2.minAreaRect(coords)\n",
    "\n",
    "#     # Get the angle from the minimum bounding rectangle\n",
    "#     angle = rect[-1]\n",
    "\n",
    "#     # Calculate the aspect ratio of the bounding rectangle\n",
    "#     width, height = rect[1]\n",
    "#     aspect_ratio = max(width, height) / min(width, height)\n",
    "\n",
    "#     # Determine the desired orientation (longer side should be horizontal)\n",
    "#     if aspect_ratio > 1:\n",
    "#         # The longer side is vertical, we need to make it horizontal\n",
    "#         if angle < -45:\n",
    "#             angle = -(90 + angle)\n",
    "#         else:\n",
    "#             angle = -angle\n",
    "#     else:\n",
    "#         # The longer side is already horizontal, we need to make it vertical\n",
    "#         angle = -(90 + angle)\n",
    "\n",
    "#     return angle\n",
    "\n",
    "# def tilt(image, angle):\n",
    "#     # Make a copy of the image to avoid modifying the original\n",
    "#     rotated_image = image.copy()\n",
    "\n",
    "#     # Rotate the copied image to deskew it\n",
    "#     (h, w) = rotated_image.shape[:2]\n",
    "#     center = (w // 2, h // 2)\n",
    "#     M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "#     rotated = cv2.warpAffine(rotated_image, M, (w, h),\n",
    "#                              flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "#     return rotated\n",
    "\n",
    "\n",
    "def get_edges(image):\n",
    "    # Convert the image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    # Get the indices of non-zero elements (edge points)\n",
    "    non_zero_indices = np.nonzero(image_array)\n",
    "\n",
    "    # Get the minimum and maximum row and column indices of non-zero elements\n",
    "    minRow, minColumn = np.min(non_zero_indices, axis=1)\n",
    "    maxRow, maxColumn = np.max(non_zero_indices, axis=1)\n",
    "\n",
    "    extLeft = minColumn\n",
    "    extRight = maxColumn\n",
    "    extTop = minRow\n",
    "    extBot = maxRow\n",
    "    return (extLeft, extRight, extBot, extTop)\n",
    "\n",
    "def get_board_mask(img):\n",
    "    color = [0, 0, 0, 255, 255, 50]\n",
    "    imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    lower = np.array(color[:3])  \n",
    "    upper = np.array(color[3:]) \n",
    "    mask = cv2.inRange(imgHSV, lower, upper)\n",
    "    return mask\n",
    "\n",
    "def get_pegs(img,x1,x2,y1,y2):\n",
    "    matrixcoor_to_realcoor = {}\n",
    "    dist_from_edge = [(x2-x1)/13,(y2-y1)/15]\n",
    "    board_width = x2-x1-2*dist_from_edge[0]\n",
    "    board_height = y2-y1-2*dist_from_edge[1]\n",
    "    horizontal_interval = board_width / 12\n",
    "    vertical_interval = board_height / 14\n",
    "    img_circle = img.copy()\n",
    "    \n",
    "    # relate matrix coordinate to real peg coordinate\n",
    "    for i in range(0,13):\n",
    "        for j in range(0,15):\n",
    "            matrixcoor_to_realcoor[i,j] = np.array([x1 + int(horizontal_interval * i + dist_from_edge[0]), y1 + int(vertical_interval * j + dist_from_edge[1])])\n",
    "    \n",
    "    for key in matrixcoor_to_realcoor:\n",
    "        cv2.circle(img_circle, matrixcoor_to_realcoor[key], 2, (200, 200, 200), -1)\n",
    "    \n",
    "    return img_circle,matrixcoor_to_realcoor\n",
    "\n",
    "def draws_pegs_on_rotated_board(image,draw_edge=False):\n",
    "    #time1=time.time()\n",
    "    boardBW = get_board_mask(image)\n",
    "    angle = whatAngle(boardBW)\n",
    "    #time2=time.time()\n",
    "    #print(\"masking and calculate angle time:\", time2-time1)\n",
    "    boardBW_tilt = tilt(boardBW, angle) \n",
    "    image_tilt = tilt(image, angle) \n",
    "    #time3=time.time()\n",
    "    #print(\"deskew time:\", time3-time2)\n",
    "    # cv2.imwrite('board_deskew.png',image_tilt)  \n",
    "            \n",
    "    #get the max edges of the board then draw edges and pegs on it\n",
    "    x1,x2,y1,y2 = get_edges(boardBW_tilt)\n",
    "    #time4=time.time()\n",
    "    #print(\"get_edges:\", time4-time3)\n",
    "    img_circle, matrixcoor_to_realcoor = get_pegs(image_tilt,x1,x2,y1,y2)\n",
    "    #time5=time.time()\n",
    "    #print(\"get_pegs:\", time5-time4)\n",
    "    \n",
    "    if draw_edge:\n",
    "        cv2.rectangle(img_circle, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "    # cv2.imwrite('board_with_pegs.png',img_circle)    \n",
    "    return matrixcoor_to_realcoor, image_tilt, img_circle\n",
    "\n",
    "def round_to_integer_with_error(float_number, error_rate = 0.4, down = True):\n",
    "    if down:\n",
    "        lower_integer = int(float_number)\n",
    "\n",
    "        # Calculate the error between the float number and the lower integer\n",
    "        error = float_number - lower_integer\n",
    "\n",
    "        # Check if the error is within the custom error rate\n",
    "        if error <= error_rate:\n",
    "            return lower_integer - 1\n",
    "        else:\n",
    "            return lower_integer \n",
    "    else:\n",
    "        upper_integer = np.ceil(float_number).astype(int)\n",
    "\n",
    "        # Calculate the error between the float number and the upper integer\n",
    "        error = upper_integer - float_number\n",
    "\n",
    "        # Check if the error is within the custom error rate\n",
    "        if error <= error_rate:\n",
    "            return upper_integer + 1\n",
    "        else:\n",
    "            return upper_integer\n",
    "\n",
    "# def translate_coordinates(r):\n",
    "#     x1,y1,x2,y2,class_id = r\n",
    "#     grid_x1 = round_to_integer_with_error(((x1-x0) / x_len) * 12,down=False)\n",
    "#     grid_y1 = round_to_integer_with_error(((y1-y0) / y_len) * 14,down=False)\n",
    "#     grid_x2 = round_to_integer_with_error(((x2-x0) / x_len) * 12)\n",
    "#     grid_y2 = round_to_integer_with_error(((y2-y0) / y_len) * 14)\n",
    "#     # if class_id == 11:\n",
    "#     #     print(\"x1:\",((x1-x0) / x_len) * 12,grid_x1)\n",
    "#     #     print(\"y1:\",((y1-y0) / y_len) * 14,grid_y1)\n",
    "#     #     print(\"x2:\",((x2-x0) / x_len) * 12,grid_x2)\n",
    "#     #     print(\"y2:\",((y2-y0) / y_len) * 14,grid_y2)\n",
    "        \n",
    "#     grid_x2 = max(grid_x1,grid_x2)\n",
    "#     grid_y2 = max(grid_y1,grid_y2)\n",
    "\n",
    "#     grid_x1 = max(0, min(grid_x1, 12 - 1))\n",
    "#     grid_y1 = max(0, min(grid_y1, 14 - 1))\n",
    "#     grid_x2 = max(0, min(grid_x2, 12 - 1))\n",
    "#     grid_y2 = max(0, min(grid_y2, 14 - 1))\n",
    "    \n",
    "#     return 12-grid_x2, 12-grid_x1,grid_y1,grid_y2\n",
    "\n",
    "def matrix_class_mapping(results,matrixcoor_to_realcoor):\n",
    "    x0,y0 = matrixcoor_to_realcoor[(0,14)]\n",
    "    matrix = np.zeros((13, 15))-1\n",
    "    x_len, y_len = np.abs(matrixcoor_to_realcoor[(0,0)]-matrixcoor_to_realcoor[(12,14)])\n",
    "    results = sorted(results,key = lambda x:x[-1])\n",
    "\n",
    "    for r in results:\n",
    "        x1,y1,x2,y2,class_id = r\n",
    "        grid_x1 = round_to_integer_with_error(((x1-x0) / x_len) * 12,down=False)\n",
    "        grid_y1 = round_to_integer_with_error(((y1-y0) / y_len) * 14,down=False)\n",
    "        grid_x2 = round_to_integer_with_error(((x2-x0) / x_len) * 12)\n",
    "        grid_y2 = round_to_integer_with_error(((y2-y0) / y_len) * 14)\n",
    "        # if class_id == 11:\n",
    "        #     print(\"x1:\",((x1-x0) / x_len) * 12,grid_x1)\n",
    "        #     print(\"y1:\",((y1-y0) / y_len) * 14,grid_y1)\n",
    "        #     print(\"x2:\",((x2-x0) / x_len) * 12,grid_x2)\n",
    "        #     print(\"y2:\",((y2-y0) / y_len) * 14,grid_y2)\n",
    "        \n",
    "        grid_x2 = max(grid_x1,grid_x2)\n",
    "        grid_y2 = max(grid_y1,grid_y2)\n",
    "\n",
    "        grid_x1 = max(0, min(grid_x1, 12 - 1))\n",
    "        grid_y1 = max(0, min(grid_y1, 14 - 1))\n",
    "        grid_x2 = max(0, min(grid_x2, 12 - 1))\n",
    "        grid_y2 = max(0, min(grid_y2, 14 - 1))\n",
    "    \n",
    "        matrix[12-grid_x2:12-grid_x1+1,grid_y1:grid_y2+1]=class_id\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "color_mapping = {\n",
    "    0: 'red', # done, battery\n",
    "    1: 'black', # board\n",
    "    2: 'green', # done, buzzer\n",
    "    3: 'orange',\n",
    "    4: 'limegreen', #done, fm\n",
    "    5: 'grey', # done (lamp; check accuracy)\n",
    "    6: 'darkred', # done, led\n",
    "    7: 'blue', # mc\n",
    "    8: 'yellow', # done, motor\n",
    "    9: 'royalblue', # done, push button\n",
    "    10: 'seagreen', # done, reed\n",
    "    11: 'firebrick', # done, speaker\n",
    "    12: 'darkgreen', # done, switch\n",
    "    13: 'purple' # done, wire\n",
    "}\n",
    "\n",
    "def show_estimated_board(results_transferred,color_mapping=color_mapping,rows = 13,cols = 15,cell_size = 50):\n",
    "    \"\"\"Draw the virtual image of the board\n",
    "\n",
    "    Args:\n",
    "        results_transferred (matrix): a matrix to store class of each block of the board\n",
    "        rows (int, optional): number of rows of the grid. Defaults to 8.\n",
    "        cols (int, optional): number of columns of the grid. Defaults to 7.\n",
    "        cell_size (int, optional): size of cell. Defaults to 50.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the total size of the image\n",
    "    image_width = cols * cell_size\n",
    "    image_height = rows * cell_size\n",
    "\n",
    "    # Create a new image with a black background\n",
    "    image = Image.new(\"RGB\", (image_width, image_height), color=\"black\")\n",
    "\n",
    "    # Create a draw object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Draw the grid with numbers\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            # Calculate the position of the top-left corner of the cell\n",
    "            x1 = col * cell_size\n",
    "            y1 = row * cell_size\n",
    "\n",
    "            # Calculate the position of the bottom-right corner of the cell\n",
    "            x2 = x1 + cell_size\n",
    "            y2 = y1 + cell_size\n",
    "\n",
    "            # Calculate the number for each cell (you can use any logic here)\n",
    "            cell_number = results_transferred[row][col]\n",
    "\n",
    "            # Draw the cell with the corresponding number\n",
    "            if cell_number >= 0:\n",
    "                draw.rectangle([x1, y1, x2, y2], fill=color_mapping[cell_number],outline='white')\n",
    "            else:\n",
    "                draw.rectangle([x1, y1, x2, y2], fill=\"black\",outline='white')\n",
    "            draw.text((x1 + 20, y1 + 20), str(cell_number),  fill=\"white\")\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the image using cv2.imread()\n",
    "# image_path = 'raw11.png'\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# # Get the necessary images and mapping using draws_pegs_on_rotated_board\n",
    "# matrixcoor_to_realcoor, image_tilt, img_circle = draws_pegs_on_rotated_board(image)\n",
    "\n",
    "# # Convert the image_tilt and img_circle (NumPy arrays) to PIL Image objects for display\n",
    "# image_tilt_pil = Image.fromarray(cv2.cvtColor(image_tilt, cv2.COLOR_BGR2RGB))\n",
    "# img_circle_pil = Image.fromarray(cv2.cvtColor(img_circle, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# # Display the images\n",
    "# image_tilt_pil.show()\n",
    "# img_circle_pil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "1. Wires not very accurate (more training or use masking)\n",
    "2. Too slow (Solved)\n",
    "3. Sometimes the shape of pieces is not correct, e.g. speaker to 2 columns\n",
    "\n",
    "Note that:\n",
    "1. When placing battery, at most at the second last column \n",
    "2. Use board with pegs to check whether the board is not skewed due to angle of the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 208.7ms\n",
      "Speed: 2.1ms preprocess, 208.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 173.5ms\n",
      "Speed: 1.8ms preprocess, 173.5ms inference, 4.2ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 170.3ms\n",
      "Speed: 1.8ms preprocess, 170.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 171.6ms\n",
      "Speed: 1.9ms preprocess, 171.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 208.0ms\n",
      "Speed: 2.1ms preprocess, 208.0ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 183.5ms\n",
      "Speed: 2.4ms preprocess, 183.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.5ms\n",
      "Speed: 1.9ms preprocess, 181.5ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.7ms\n",
      "Speed: 1.9ms preprocess, 178.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.8ms\n",
      "Speed: 2.1ms preprocess, 182.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.5ms\n",
      "Speed: 1.8ms preprocess, 181.5ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.3ms\n",
      "Speed: 2.2ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.3ms\n",
      "Speed: 1.8ms preprocess, 177.3ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.9ms\n",
      "Speed: 1.8ms preprocess, 180.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.3ms\n",
      "Speed: 1.8ms preprocess, 179.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.6ms\n",
      "Speed: 2.1ms preprocess, 179.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.4ms\n",
      "Speed: 2.1ms preprocess, 181.4ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.0ms\n",
      "Speed: 1.8ms preprocess, 180.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.1ms\n",
      "Speed: 1.8ms preprocess, 179.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.1ms\n",
      "Speed: 1.7ms preprocess, 180.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.1ms\n",
      "Speed: 2.0ms preprocess, 179.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.2ms\n",
      "Speed: 2.0ms preprocess, 178.2ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 195.8ms\n",
      "Speed: 1.7ms preprocess, 195.8ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.4ms\n",
      "Speed: 1.9ms preprocess, 180.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "/opt/homebrew/lib/python3.11/site-packages/google/protobuf/symbol_database.py:78: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "\n",
      "0: 736x800 1 battery, 3 wires, 181.7ms\n",
      "Speed: 1.9ms preprocess, 181.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.9ms\n",
      "Speed: 1.9ms preprocess, 180.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.2ms\n",
      "Speed: 2.2ms preprocess, 180.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.7ms\n",
      "Speed: 1.9ms preprocess, 178.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.0ms\n",
      "Speed: 1.7ms preprocess, 178.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.6ms\n",
      "Speed: 1.9ms preprocess, 178.6ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.2ms\n",
      "Speed: 1.8ms preprocess, 181.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.3ms\n",
      "Speed: 2.0ms preprocess, 181.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.4ms\n",
      "Speed: 1.8ms preprocess, 179.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.8ms\n",
      "Speed: 1.9ms preprocess, 180.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.8ms\n",
      "Speed: 1.9ms preprocess, 180.8ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 176.2ms\n",
      "Speed: 1.9ms preprocess, 176.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.2ms\n",
      "Speed: 2.1ms preprocess, 181.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 187.7ms\n",
      "Speed: 1.9ms preprocess, 187.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.5ms\n",
      "Speed: 1.8ms preprocess, 179.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.6ms\n",
      "Speed: 1.8ms preprocess, 179.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.6ms\n",
      "Speed: 1.8ms preprocess, 181.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.3ms\n",
      "Speed: 2.1ms preprocess, 181.3ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.6ms\n",
      "Speed: 1.8ms preprocess, 179.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.6ms\n",
      "Speed: 2.0ms preprocess, 180.6ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 176.9ms\n",
      "Speed: 1.9ms preprocess, 176.9ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.6ms\n",
      "Speed: 1.8ms preprocess, 179.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.8ms\n",
      "Speed: 1.9ms preprocess, 177.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.0ms\n",
      "Speed: 1.8ms preprocess, 181.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 186.4ms\n",
      "Speed: 1.9ms preprocess, 186.4ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 185.8ms\n",
      "Speed: 1.9ms preprocess, 185.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.3ms\n",
      "Speed: 1.8ms preprocess, 179.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.9ms\n",
      "Speed: 1.8ms preprocess, 178.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.7ms\n",
      "Speed: 2.1ms preprocess, 181.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.3ms\n",
      "Speed: 1.9ms preprocess, 181.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 176.7ms\n",
      "Speed: 2.1ms preprocess, 176.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.4ms\n",
      "Speed: 1.8ms preprocess, 179.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.8ms\n",
      "Speed: 2.0ms preprocess, 179.8ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.2ms\n",
      "Speed: 1.8ms preprocess, 181.2ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.4ms\n",
      "Speed: 1.9ms preprocess, 181.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.3ms\n",
      "Speed: 1.9ms preprocess, 179.3ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.0ms\n",
      "Speed: 1.9ms preprocess, 181.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.9ms\n",
      "Speed: 1.8ms preprocess, 180.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.9ms\n",
      "Speed: 1.9ms preprocess, 179.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.5ms\n",
      "Speed: 2.1ms preprocess, 180.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.5ms\n",
      "Speed: 1.8ms preprocess, 181.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.8ms\n",
      "Speed: 1.8ms preprocess, 180.8ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 2 wires, 180.3ms\n",
      "Speed: 2.0ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.3ms\n",
      "Speed: 2.1ms preprocess, 182.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.7ms\n",
      "Speed: 2.0ms preprocess, 179.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 183.7ms\n",
      "Speed: 1.7ms preprocess, 183.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.5ms\n",
      "Speed: 1.9ms preprocess, 179.5ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.7ms\n",
      "Speed: 2.2ms preprocess, 180.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.1ms\n",
      "Speed: 1.8ms preprocess, 180.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.0ms\n",
      "Speed: 1.8ms preprocess, 180.0ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.8ms\n",
      "Speed: 2.0ms preprocess, 178.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.5ms\n",
      "Speed: 1.9ms preprocess, 178.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.4ms\n",
      "Speed: 1.8ms preprocess, 179.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.6ms\n",
      "Speed: 1.9ms preprocess, 178.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.8ms\n",
      "Speed: 1.8ms preprocess, 179.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.9ms\n",
      "Speed: 1.8ms preprocess, 181.9ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.1ms\n",
      "Speed: 2.1ms preprocess, 180.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.4ms\n",
      "Speed: 1.9ms preprocess, 179.4ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.0ms\n",
      "Speed: 1.8ms preprocess, 179.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.8ms\n",
      "Speed: 2.1ms preprocess, 179.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.9ms\n",
      "Speed: 2.1ms preprocess, 177.9ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.3ms\n",
      "Speed: 2.3ms preprocess, 180.3ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.2ms\n",
      "Speed: 2.0ms preprocess, 179.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.4ms\n",
      "Speed: 1.9ms preprocess, 178.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.7ms\n",
      "Speed: 1.8ms preprocess, 178.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.6ms\n",
      "Speed: 2.1ms preprocess, 178.6ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.2ms\n",
      "Speed: 2.0ms preprocess, 180.2ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.0ms\n",
      "Speed: 1.9ms preprocess, 179.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.1ms\n",
      "Speed: 2.1ms preprocess, 179.1ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.5ms\n",
      "Speed: 2.2ms preprocess, 177.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.6ms\n",
      "Speed: 1.9ms preprocess, 180.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.1ms\n",
      "Speed: 2.0ms preprocess, 182.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.3ms\n",
      "Speed: 2.3ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.0ms\n",
      "Speed: 2.1ms preprocess, 180.0ms inference, 0.9ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.7ms\n",
      "Speed: 1.9ms preprocess, 182.7ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.4ms\n",
      "Speed: 1.8ms preprocess, 182.4ms inference, 1.2ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.2ms\n",
      "Speed: 1.9ms preprocess, 181.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.2ms\n",
      "Speed: 1.8ms preprocess, 182.2ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.6ms\n",
      "Speed: 2.1ms preprocess, 181.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.2ms\n",
      "Speed: 1.8ms preprocess, 180.2ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.6ms\n",
      "Speed: 1.9ms preprocess, 179.6ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.7ms\n",
      "Speed: 1.8ms preprocess, 178.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.3ms\n",
      "Speed: 1.8ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.4ms\n",
      "Speed: 1.9ms preprocess, 177.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.6ms\n",
      "Speed: 1.8ms preprocess, 181.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.8ms\n",
      "Speed: 1.8ms preprocess, 180.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.0ms\n",
      "Speed: 1.9ms preprocess, 180.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.8ms\n",
      "Speed: 2.1ms preprocess, 178.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.2ms\n",
      "Speed: 1.8ms preprocess, 179.2ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.0ms\n",
      "Speed: 1.8ms preprocess, 180.0ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.4ms\n",
      "Speed: 1.8ms preprocess, 182.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.1ms\n",
      "Speed: 2.0ms preprocess, 180.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.5ms\n",
      "Speed: 1.8ms preprocess, 180.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.6ms\n",
      "Speed: 2.0ms preprocess, 179.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.6ms\n",
      "Speed: 1.8ms preprocess, 181.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.8ms\n",
      "Speed: 1.7ms preprocess, 180.8ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 185.5ms\n",
      "Speed: 2.0ms preprocess, 185.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.8ms\n",
      "Speed: 1.8ms preprocess, 181.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.6ms\n",
      "Speed: 2.2ms preprocess, 178.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.7ms\n",
      "Speed: 1.8ms preprocess, 179.7ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.3ms\n",
      "Speed: 1.9ms preprocess, 181.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.7ms\n",
      "Speed: 1.8ms preprocess, 181.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.1ms\n",
      "Speed: 1.9ms preprocess, 180.1ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.5ms\n",
      "Speed: 1.9ms preprocess, 179.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.9ms\n",
      "Speed: 1.8ms preprocess, 180.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.6ms\n",
      "Speed: 2.1ms preprocess, 182.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.6ms\n",
      "Speed: 2.1ms preprocess, 178.6ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.1ms\n",
      "Speed: 1.8ms preprocess, 182.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.0ms\n",
      "Speed: 2.3ms preprocess, 180.0ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.4ms\n",
      "Speed: 2.1ms preprocess, 178.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.9ms\n",
      "Speed: 2.0ms preprocess, 180.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.2ms\n",
      "Speed: 2.1ms preprocess, 179.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.3ms\n",
      "Speed: 1.9ms preprocess, 179.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 185.3ms\n",
      "Speed: 1.9ms preprocess, 185.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.1ms\n",
      "Speed: 2.2ms preprocess, 181.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.8ms\n",
      "Speed: 1.9ms preprocess, 179.8ms inference, 0.8ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.9ms\n",
      "Speed: 1.8ms preprocess, 182.9ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.3ms\n",
      "Speed: 1.9ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 183.1ms\n",
      "Speed: 1.8ms preprocess, 183.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.7ms\n",
      "Speed: 2.0ms preprocess, 181.7ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.1ms\n",
      "Speed: 2.1ms preprocess, 180.1ms inference, 0.6ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.9ms\n",
      "Speed: 1.8ms preprocess, 179.9ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.4ms\n",
      "Speed: 2.1ms preprocess, 180.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.0ms\n",
      "Speed: 2.4ms preprocess, 180.0ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.5ms\n",
      "Speed: 1.7ms preprocess, 180.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.8ms\n",
      "Speed: 2.0ms preprocess, 178.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 176.5ms\n",
      "Speed: 1.9ms preprocess, 176.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.2ms\n",
      "Speed: 2.2ms preprocess, 180.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.6ms\n",
      "Speed: 1.7ms preprocess, 179.6ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.4ms\n",
      "Speed: 1.8ms preprocess, 179.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.8ms\n",
      "Speed: 1.9ms preprocess, 178.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.4ms\n",
      "Speed: 2.0ms preprocess, 180.4ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 176.4ms\n",
      "Speed: 1.9ms preprocess, 176.4ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.5ms\n",
      "Speed: 2.2ms preprocess, 180.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.3ms\n",
      "Speed: 2.1ms preprocess, 182.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.3ms\n",
      "Speed: 2.1ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.5ms\n",
      "Speed: 2.2ms preprocess, 177.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.8ms\n",
      "Speed: 1.8ms preprocess, 177.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.7ms\n",
      "Speed: 1.9ms preprocess, 177.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.4ms\n",
      "Speed: 2.1ms preprocess, 181.4ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.2ms\n",
      "Speed: 1.9ms preprocess, 181.2ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.9ms\n",
      "Speed: 1.8ms preprocess, 179.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 179.8ms\n",
      "Speed: 2.1ms preprocess, 179.8ms inference, 0.4ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.6ms\n",
      "Speed: 2.1ms preprocess, 180.6ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.1ms\n",
      "Speed: 1.9ms preprocess, 180.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.5ms\n",
      "Speed: 2.0ms preprocess, 178.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.9ms\n",
      "Speed: 1.8ms preprocess, 177.9ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 180.3ms\n",
      "Speed: 1.8ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 181.7ms\n",
      "Speed: 1.9ms preprocess, 181.7ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 178.3ms\n",
      "Speed: 2.2ms preprocess, 178.3ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 182.3ms\n",
      "Speed: 2.1ms preprocess, 182.3ms inference, 0.9ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 195.1ms\n",
      "Speed: 1.9ms preprocess, 195.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 188.2ms\n",
      "Speed: 2.0ms preprocess, 188.2ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 189.5ms\n",
      "Speed: 2.0ms preprocess, 189.5ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 184.9ms\n",
      "Speed: 1.8ms preprocess, 184.9ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 175.1ms\n",
      "Speed: 1.8ms preprocess, 175.1ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 177.0ms\n",
      "Speed: 2.2ms preprocess, 177.0ms inference, 0.7ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n",
      "0: 736x800 1 battery, 1 speaker, 3 wires, 190.8ms\n",
      "Speed: 1.8ms preprocess, 190.8ms inference, 0.5ms postprocess per image at shape (1, 3, 736, 800)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m     cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    117\u001b[0m \u001b[39m# Assuming you have the matrix results_transferred ready\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m draw_virtual_board_video(source\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mraw_videos/raw_video_5.mp4\u001b[39;49m\u001b[39m'\u001b[39;49m, show_pegs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    120\u001b[0m \u001b[39m# draw_virtual_board_video(source=0,show_pegs=True,store=True)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 77\u001b[0m, in \u001b[0;36mdraw_virtual_board_video\u001b[0;34m(source, show_pegs, print_time, frame_rate, store)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDraw pegs uses:\u001b[39m\u001b[39m\"\u001b[39m, draw_peg_time \u001b[39m-\u001b[39m start_time)\n\u001b[1;32m     76\u001b[0m \u001b[39m# Use YOLO object detection to get position\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(frame_tilt,show\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,conf\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m print_time:\n\u001b[1;32m     79\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYOLO object detection uses:\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mdraw_peg_time)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/yolo/engine/model.py:255\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m overrides \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m overrides:\n\u001b[1;32m    254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39msave_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mget_save_dir()\n\u001b[0;32m--> 255\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/yolo/engine/predictor.py:195\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/yolo/engine/predictor.py:251\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 251\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(im, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    253\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/yolo/engine/predictor.py:137\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference\u001b[39m(\u001b[39mself\u001b[39m, im, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    135\u001b[0m     visualize \u001b[39m=\u001b[39m increment_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_dir \u001b[39m/\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstem,\n\u001b[1;32m    136\u001b[0m                                mkdir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mvisualize \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type\u001b[39m.\u001b[39mtensor) \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:330\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    327\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[1;32m    331\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 82\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     83\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:42\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def draw_virtual_board_video(source=0,show_pegs=False,print_time=False,frame_rate=5,store=False):\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    model = YOLO('best (8).pt')\n",
    "    i = 0\n",
    "    prev = 0\n",
    "    \n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands()\n",
    "    \n",
    "    if store:\n",
    "        # Create the output folder if it doesn't exist\n",
    "        if not os.path.exists('raw_videos'):\n",
    "            os.makedirs('raw_videos')\n",
    "            \n",
    "        # Get the list of existing files in the \"cropped_frames\" directory\n",
    "        existing_files = os.listdir('raw_videos')\n",
    "        if existing_files:\n",
    "            video_count = max([int(file_name.split('_')[2].split('.')[0]) for file_name in existing_files]) + 1\n",
    "        else:\n",
    "            video_count = 0\n",
    "        \n",
    "        # Create a VideoWriter to save the cropped video\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "        output_path = os.path.join('raw_videos', f\"raw_video_{video_count}.mp4\")\n",
    "        writer = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        # Capture a frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Camera Connection Failed\")\n",
    "            break\n",
    "        \n",
    "        if store:\n",
    "            writer.write(frame)\n",
    "\n",
    "        # Read the video properties to get the frame width and height\n",
    "        crop_x, crop_y, crop_width, crop_height = 760, 250, 450, 500\n",
    "        frame_for_veri = frame[crop_y:crop_y + crop_height, crop_x:crop_x + crop_width]\n",
    "        frame_for_hand = frame[crop_y:crop_y + crop_height, crop_x:crop_x + crop_width+100]\n",
    "        frame = cv2.rotate(frame_for_veri, cv2.ROTATE_90_CLOCKWISE)\n",
    "        \n",
    "        # Detecting hand\n",
    "        results = hands.process(cv2.cvtColor(frame_for_hand, cv2.COLOR_BGR2RGB))\n",
    "        if results.multi_hand_landmarks:\n",
    "            # If a hand is detected, skip this frame\n",
    "            continue\n",
    "        \n",
    "        time_elapsed = time.time() - prev\n",
    "        if time_elapsed > 1./frame_rate:\n",
    "            prev = time.time()\n",
    "        \n",
    "            # Convert the raw frame to PIL Image\n",
    "            #raw_frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            # Reflect the raw frame horizontally and Rotate the raw frame 90 degrees counterclockwise\n",
    "            #raw_frame_pil = raw_frame_pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            #raw_frame_pil = raw_frame_pil.rotate(90, expand=True)\n",
    "            \n",
    "            # Convert the frame to RGB for PIL (optional)\n",
    "            # frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            start_time = time.time()\n",
    "            if print_time:\n",
    "                print(f\"Frame {i} starts\")\n",
    "            \n",
    "            # Detect hands in the frame and Check if any hands are detected\n",
    "            \n",
    "            \n",
    "            # Draw pegs, return a mapping between matrix and real coordinates\n",
    "            matrixcoor_to_realcoor, frame_tilt, frame_circle = draws_pegs_on_rotated_board(frame)\n",
    "            if print_time:\n",
    "                draw_peg_time = time.time()\n",
    "                print(\"Draw pegs uses:\", draw_peg_time - start_time)\n",
    "\n",
    "            # Use YOLO object detection to get position\n",
    "            results = model.predict(frame_tilt,show=True,conf=0.3)\n",
    "            if print_time:\n",
    "                print(\"YOLO object detection uses:\", time.time()-draw_peg_time)\n",
    "            \n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                output = np.hstack([boxes.xyxy, boxes.cls[:, np.newaxis]])\n",
    "\n",
    "            # Get the mapping between matrix entries and class, then draw the virtual board\n",
    "            matrix = matrix_class_mapping(output, matrixcoor_to_realcoor)\n",
    "            final_image = show_estimated_board(matrix)\n",
    "\n",
    "            # Convert the image back to BGR format for displaying with OpenCV\n",
    "            final_image_bgr = cv2.cvtColor(np.array(final_image), cv2.COLOR_RGB2BGR)\n",
    "            #raw_frame_pil = cv2.cvtColor(np.array(raw_frame_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Display the raw board and virtual board outputs in separate windows\n",
    "            # cv2.imshow(\"Raw Board\", np.array(raw_frame_pil))\n",
    "            cv2.imshow(\"Virtual Board\", final_image_bgr)\n",
    "            if show_pegs:\n",
    "                cv2.imshow(\"Board with Pegs\",frame_circle)\n",
    "            \n",
    "            if print_time:\n",
    "                end_time = time.time()\n",
    "                time_elapsed = end_time - start_time\n",
    "                print(\"Frame {i} ends, using:\", time_elapsed)\n",
    "            i += 1\n",
    "            # Wait for the specified interval time\n",
    "            #time.sleep(interval_seconds)\n",
    "\n",
    "        # Check for the 'q' key press to exit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close the windows\n",
    "    cap.release()\n",
    "    if store:\n",
    "        writer.release() \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Assuming you have the matrix results_transferred ready\n",
    "draw_virtual_board_video(source='raw_videos/raw_video_5.mp4', show_pegs=True)\n",
    "\n",
    "# draw_virtual_board_video(source=0,show_pegs=True,store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# Display the frame\u001b[39;00m\n\u001b[1;32m     35\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mFrame\u001b[39m\u001b[39m'\u001b[39m, frame)\n\u001b[0;32m---> 37\u001b[0m time\u001b[39m.\u001b[39msleep(delay)\n\u001b[1;32m     39\u001b[0m \u001b[39m# Exit the loop if 'q' key is pressed\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize mediapipe hands module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Open a video capture object\n",
    "video_capture = cv2.VideoCapture('raw_videos/raw_video_5.mp4')  # Replace with your video file path\n",
    "delay = 0.2  # For example, 0.1 seconds\n",
    "\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Read the video properties to get the frame width and height\n",
    "    crop_x, crop_y, crop_width, crop_height = 760, 250, 550, 500\n",
    "    frame = frame[crop_y:crop_y + crop_height, crop_x:crop_x + crop_width]\n",
    "        \n",
    "    # Convert the frame to RGB (mediapipe uses RGB images)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect hands in the frame\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        # If a hand is detected, put text on the frame\n",
    "        #cv2.putText(frame, 'Hand Detected', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    # Exit the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close any open windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
